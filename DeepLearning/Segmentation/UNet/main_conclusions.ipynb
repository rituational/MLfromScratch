{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## U Net Paper\r\n",
    "\r\n",
    "### Note: Working colab code @ https://drive.google.com/drive/folders/1jw0KE6uZ0a625Rniy6xxB_8GzKK3c1FZ?usp=sharing\r\n",
    "\r\n",
    "\r\n",
    "<img src=\"./helper/1.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>\r\n",
    "\r\n",
    "**Note:It is fully convolutional, no fully connected layer used** \r\n",
    "* Input: Grayscale 572*572 \r\n",
    "* Output: 388*388 2 output classes\r\n",
    "* At every level two 3*3 **valid** convolution \r\n",
    "* * Valid convolution (no padding): 2nd layer shape = (572-3)/1+1 = 570  {(input-filter)/stride+1 }\r\n",
    "* downsample 2*2 stride 2 downsampling\r\n",
    "* Upsample using transpose convolution \r\n",
    "* * The resulting layer is a concatenation of \r\n",
    "* * * skip connection (we need to crop the image in the left of U to match dimension in the output upsampled layer)\r\n",
    "* * * Upsampling\r\n",
    "\r\n",
    "* final 1*1 conv to get to number of classes\r\n",
    "\r\n",
    "\r\n",
    "* The downsample gives what we are looking at. Upsample (with skip connection) tells where it is.\r\n",
    "* For a large image, they take tiles, put padding with mirroring strategy (so that context is there). Then stitch the results together \r\n",
    "\r\n",
    "#### Why not 3*3 same convolutions and preserve the feature map shape rather than down and up sample? \r\n",
    "* The receptive field doesnt grow that fast \r\n",
    "* Computationally expensive"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "## From Implementation \r\n",
    "(found @ model.py)\r\n",
    "\r\n",
    "At every stage\r\n",
    "* We have two 3*3 convolution layers (stride = 1) which does valid convolution. For ease in implementation, used 'same' convolution (padding = 1). Also, add extra BN layer. \r\n",
    "* we put bias = false as we use BN which will cancel the bias \r\n",
    "* For first stage in_channels = 1, out_channels = 64, for second in_channels = 64, out_channels = 128"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./helper/1.PNG)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DoubleConv(nn.Module):\r\n",
    "    def __init__(self, in_channels, out_channels):\r\n",
    "        super(DoubleConv, self).__init__()\r\n",
    "        self.conv = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1 ,1 , bias = False), # in_ch, out_ch, kernel_size, stride, padding\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLu(inplace = True), \r\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1 ,1 , bias = False),\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLu(inplace = True) \r\n",
    "\r\n",
    "        )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## For the down part of U net: \r\n",
    "* We need the 2 convolutions and then maxpooling to be repeated 4 times. The out_channels at these stages be : 64,128,256,512\r\n",
    "\r\n",
    "* * We make a list with function that creates 2 convolution layers of required dimensions \r\n",
    "* * * 1st entry (1st stage): in_channels = 1, out_channels = 64 (1st in the list features)\r\n",
    "* * * 2nd entry (2nd stage): in_channels = 64 (prev feature), out_channels = 128 (2nd in the list features) and so on\r\n",
    "* * We loop through this list and \r\n",
    "* * * Make 2 layers of conv (DoubleConv fn)\r\n",
    "* * * Downsample (maxpool)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features =[64,128,256,512] # the out_channels/filters at every stage as we go down U net\r\n",
    "\r\n",
    "self.downs = nn.ModuleList()\r\n",
    "self.pools = nn.MaxPool2d(kernel_size = 2, stride = 2)\r\n",
    "\r\n",
    "for feature in features:\r\n",
    "    self.downs.append(DoubleConv(in_channels, feature))\r\n",
    "    in_channels = feature\r\n",
    "\r\n",
    "# let x be the input passed to this function \r\n",
    "\r\n",
    "for down in self.downs:\r\n",
    "    x = down(x)     # 2 layers of conv\r\n",
    "    x = self.pool(x)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need bottleneck layer which is in_channels = 512 (features[-1]), out_channels = 1024"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "self.bottleneck = DoubleConv(features[-1], features[-1]*2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## For up part of U Net \r\n",
    "\r\n",
    "* We need to make a list of skip connections (They are the output from last conv layer in each down stage)\r\n",
    "* * Reverse the list (since last skip connection created going down gets used first when going up the U net)\r\n",
    "\r\n",
    "* As we go up out_channels are 512, 256, 128, 64 (i.e. reversed(features))\r\n",
    "\r\n",
    "In the implementation below:\r\n",
    "* * We make a list with transpose2d, DoubleConv2d of required dimension\r\n",
    "* * We create a new loop, we perform two entries in the list above in one iteration \r\n",
    "* * * We do transpose2d (upsample)\r\n",
    "* * * concatenate result with skip connection (reversed list)\r\n",
    "* * * maxpooling and upsampling can reduce the dimensions (eg input size = 161, after 4 stages botteleneck is 40, when we upsample it is 160: 1 pixel less: concatenation of skip + upsample not possible (Paper crops the skip connection to match upsampled output. Here resize our skip connection before concatenation)\r\n",
    "* * * pass concatenated through Double Conv (2 conv layers)\r\n",
    "\r\n",
    "             "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### make list of skip connections and reverse\r\n",
    " \r\n",
    "skip_connections = [] \r\n",
    "\r\n",
    "for down in self.downs: # (shown separately just for illustration, else cache during going down )\r\n",
    "    x = down(x)     # down is basically 2 conv layers\r\n",
    "    skip_connections.append(x)\r\n",
    " \r\n",
    "skip_connections = skip_connections[::-1]\r\n",
    "\r\n",
    "\r\n",
    "### list with transpose2d, DoubleConv2d of required dimension\r\n",
    "\r\n",
    "self.ups = nn.ModuleList()\r\n",
    "\r\n",
    "for feature in reversed(features):\r\n",
    "    self.ups.append(\r\n",
    "        nn.ConvTranspose2d(\r\n",
    "            feature*2, feature, kernel_size = 2, stride = 2,\r\n",
    "        )\r\n",
    "    )\r\n",
    "    self.ups.append(DoubleConv(feature*2, feature))\r\n",
    "        \r\n",
    "## Create new loop, we perform two entries in the list above in one iteration \r\n",
    "\r\n",
    "for idx in range(0, len(self.ups), 2):\r\n",
    "    x = self.ups[idx](x)    # upsample( transpose)\r\n",
    "\r\n",
    "    skip_connection = skip_connections[idx//2] # 2 steps in self.ups, we only have one skip per stage\r\n",
    "\r\n",
    "    if x.shape != skip_connection.shape: \r\n",
    "                x = TF.resize(x, size = skip_connection.shape[2:])\r\n",
    "\r\n",
    "\r\n",
    "    concat_skip = torch.cat((skip_connection, x), dim = 1) # concat along the depth\r\n",
    "\r\n",
    "    x = self.ups[idx+1](concat_skip) #2 layers of conv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss function and training \r\n",
    "\r\n",
    "@ dataset.py \r\n",
    "\r\n",
    "We convert mask to binary {0,1} so that it can be used as logits ground truth in our BCE loss later. \r\n",
    "\r\n",
    "@ train.py \r\n",
    "\r\n",
    "We use BCEwithlogit loss (this passes our predictions through sigmoid and then applies BCE on it vs target) \r\n",
    "#### Note\r\n",
    "BCE is acting on this 2D array of mask and predicted image. (default it takse mean to reduce the array i believe)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ref: \r\n",
    "Learnings from implementation found @ https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/image_segmentation/semantic_segmentation_unet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BCE nuances \r\n",
    "\r\n",
    "* BCE is used as method of reconstruction (autoencoder, segmentation)\r\n",
    "* BCE is calculated for each pixel according to usual logit loss formula. Then we can reduce it by sum/ mean (default)\r\n",
    "* We check the same below \r\n",
    "* * (BCEwithlogits -> We dont apply sigmoid activation to final layer (same done in scratch implementation) )\r\n",
    "* * Apply sigmoid, then take BCEloss \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch \r\n",
    "\r\n",
    "pred = torch.randn((3, 1, 160, 240)) # output of last layer without activation (linear activation??)\r\n",
    "target = torch.randint(low=0, high=2, size = (3, 1, 160, 240), dtype=torch.float32 ) # (0,1 mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\r\n",
    "criterion(pred, target)\r\n",
    "\r\n",
    "# this gives output tensor(0.8079)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m = torch.nn.Sigmoid()\r\n",
    "loss = torch.nn.BCELoss()\r\n",
    "\r\n",
    "loss(m(pred), target)\r\n",
    "\r\n",
    "# m(pred) brings it to [0,1]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit (conda)"
  },
  "interpreter": {
   "hash": "6a33784d2e02aee83e49ac6533fd5586adcd80ae4d6db07a7c73263e9bf81256"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
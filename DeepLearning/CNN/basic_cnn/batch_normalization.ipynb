{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of inputs to deeper layers may vary after weigt update after every mini batch.  This can cause the learning algorithm to forever chase a moving target. (interal covariate shift)\n",
    "\n",
    "BN standardizes input to layer for each mini batch, stabilizing the learning process and faster convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm\n",
    "Step 1: zero center & Normalize input\n",
    "\n",
    "Step 2: Scale & Shift using two new parameter\n",
    "\n",
    "Steps:\n",
    "\n",
    "$\\mu = \\frac{1}{n} \\sum x $\n",
    "\n",
    "$ \\sigma ^2 = \\frac{1}{n} \\sum (x - \\mu)^2 $\n",
    "\n",
    "$ \\hat x = \\frac {x - \\mu}{\\sqrt{\\sigma ^2 + \\epsilon}} $\n",
    "\n",
    "$ z = \\gamma * \\hat x + \\beta $\n",
    "Batch Norm is done in batch of training data, hence the name. the \"n\" in calculation represents the batch size\n",
    "\n",
    "$\\gamma$ -> output scale parametet, $\\beta$ -> output offset/shift parameter\n",
    "\n",
    "z -> rescaled and shifted version of inputs\n",
    "\n",
    "Testing\n",
    "When working with batches, the mean and sigma can vary lot\n",
    "\n",
    "One solution : run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer.\n",
    "\n",
    "estimate these final statistics during training by using a moving average of layerâ€™s input means and standard deviations.\n",
    "\n",
    "Hence, four vectors are learned at each batch-norm layer:\n",
    "\n",
    "$\\gamma$ : output scale vector\n",
    "$\\beta$ : output offset vector\n",
    "$\\mu$ : final input mean vector (exp mov avg)\n",
    "$\\sigma$ : final input std dev vector (exp mov avg)\n",
    "The $\\mu, \\sigma$ are estimated only at training and used only at testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary:\n",
    "\n",
    "It had solved vanishing/exploding grads even with saturating activation functions (sigmoid and logistic)\n",
    "\n",
    "Faster convergence but training is slow (bcos more computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(20, activation='elu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the batchnorm has 4 parameters per input feature. Here the input is 784 features\n",
    "\n",
    "Implies 4*784 = 3136\n",
    "\n",
    "The $\\mu, \\sigma$ at bach norm layer is not used during back-prop, hence it is shown as non-trainable params.\n",
    "\n",
    "ie 2*784 = 1568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to add Batch Norm layer\n",
    "The authors of the BN paper argued in favor of adding the BN layers before the activation functions (for Relu), rather than after. For sigmoid, tanh BN should be used for the activation. (a/c to machinelearningmastery)\n",
    "\n",
    "Depends on our dataset - needs experimentation\n",
    "\n",
    "To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers.\n",
    "Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass use_bias=False when creating it)\n",
    "\n",
    "BN provides some level of regularization, hence dont use with dropout. Dropout introduces randomness that can confuse BN by adding noise to the normalization.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html (for back prop derivation )\n",
    "https://github.com/wiseodd/hipsternet/blob/master/hipsternet/layer.py (code below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_forward(X, gamma, beta, cache, momentum=.9, train=True):\n",
    "    running_mean, running_var = cache\n",
    "\n",
    "    if train:\n",
    "        mu = np.mean(X, axis=0)\n",
    "        var = np.var(X, axis=0)\n",
    "\n",
    "        X_norm = (X - mu) / np.sqrt(var + c.eps)\n",
    "        out = gamma * X_norm + beta\n",
    "\n",
    "        cache = (X, X_norm, mu, var, gamma, beta)\n",
    "\n",
    "        running_mean = util.exp_running_avg(running_mean, mu, momentum)\n",
    "        running_var = util.exp_running_avg(running_var, var, momentum)\n",
    "    else:\n",
    "        X_norm = (X - running_mean) / np.sqrt(running_var + c.eps)\n",
    "        out = gamma * X_norm + beta\n",
    "        cache = None\n",
    "        \n",
    "    return out, cache, running_mean, running_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.paperspace.com/busting-the-myths-about-batch-normalization/ \n",
    "BN gives us control on the output distribution of each layer. \n",
    "\n",
    "\n",
    "BN helps maintain the input to the linear region of the activation? So even sigmoid can be used in a better way? Doubt!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

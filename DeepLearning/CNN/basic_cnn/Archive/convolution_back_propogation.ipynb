{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "Amazing explanation of backpropogation:\n",
    "https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c\n",
    "\n",
    "![title](helper\\backpropagation\\1.gif)\n",
    "![title](helper\\backpropagation\\2.JPG)\n",
    "![title](helper\\backpropagation\\3.JPG)\n",
    "![title](helper\\backpropagation\\4.JPG)\n",
    "![title](helper\\backpropagation\\5.JPG)\n",
    "![title](helper\\backpropagation\\6.GIF)\n",
    "![title](helper\\backpropagation\\7.JPG)\n",
    "![title](helper\\backpropagation\\8.JPG)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From Andrew Ng's "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-47-e2ca1f9aa0ff>, line 8)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-47-e2ca1f9aa0ff>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    n_H, n_W = Z.shape\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "   (Z, W, b, hparameters) = cache \n",
    "\n",
    "   stride = hparameters['stride']\n",
    "   pad = hparameters['pad']\n",
    "\n",
    "   (f, f, n_C_prev, n_C) = W.shape\n",
    "    n_H, n_W = Z.shape\n",
    "\n",
    "    # A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a = A[i,:,:,:]\n",
    "        for c in range(n_C):\n",
    "            for h in range(n_H):\n",
    "                for w in range(n_W):\n",
    "                    \n",
    "                    d_A = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv_backward(dZ, cache):\n",
    "#     (A_prev, W, b, hparameters) = cache # is this cache for prev layer? Then W are for previous .. doesnt make sense. We are finding dW for current layer\n",
    "#     (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "#     stride = hparameters['stride']\n",
    "#     pad = hparameters['pad']\n",
    "\n",
    "#     (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "#     m, n_H, n_W, n_C = dZ.shape  \n",
    "\n",
    "#     dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "#     db = np.zeros((1,1,1,n_C))\n",
    "#     dA_prev = np.zeros(A_prev.shape)\n",
    "\n",
    "#     A_prev_pad = zero_pad(A_prev, pad)\n",
    "#     dA_prev_pad = zero_pad(dA_prev,pad)\n",
    "\n",
    "#     for i in range(m):\n",
    "#         a_prev_pad = A_prev_pad[i]\n",
    "#         da_prev_pad = dA_prev_pad[i]\n",
    "#         for h in range(n_H):\n",
    "#             for w in range(n_W):\n",
    "#                 for c in range(n_C):  # we calculate the each filter seperately\n",
    "#                     vert_start = h * stride\n",
    "\n",
    "#                     vert_end = vert_start + f\n",
    "#                     horiz_start = w * stride\n",
    "\n",
    "#                     horiz_end = horiz_start + f\n",
    "\n",
    "#                     a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "#                     da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "#                     dW[:,:,:,c] += a_slice * dZ[i, h, w, c]#we take the whole 3D a slice like we did in forward prop \n",
    "#                     db[:,:,:,c] += dZ[i, h, w, c] \n",
    "        \n",
    "#         dA_prev[i,:,:,:] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "\n",
    "#         return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conv_forward import conv_forward, zero_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dA_mean = 0.028568851249485317\ndW_mean = 0.6273265977277442\ndb_mean = -1.2449234461925227\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "# print(dA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
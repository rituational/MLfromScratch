{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "![](helper/1.JPG)\n",
    "\n",
    "Input shape (m,28,28)\n",
    "\n",
    "\n",
    "1 layer: conv with 3*3 filter, stride = 1, pad = 0 . \n",
    "\n",
    "        out_height = (28-3)/1 + 1 = 26             [(image_size-filter_size+2*pad)/stride + 1]\n",
    "        out_dimension = (m,26,26,8)\n",
    "        Weight shape = (3,3,8)                     In case there was preceding conv layer. Weight shape = (3,3, n_prev_filters, n_next_filters)\n",
    "\n",
    "2 layer: max pool 2*2 filter  stride = 2\n",
    "\n",
    "        out_height = (26-2)/2 + 1 = 13              [(image_size-filter_size)/stride + 1]\n",
    "        out_dimension = (m,13,13,8)                   Maxpool is just downsampling, no of filters remain the same.\n",
    "\n",
    "3 layer: softmax (with flatten)\n",
    "        \n",
    "        intermediate_length = (m,13*13*8)              conv layer flattened into a 1D array \n",
    "        out_dimension = (m,10)\n",
    "        Weight shape = (13*13*8, 10)               weight shape is always n_previous_layer, n_next layer\n",
    "        bias shape = (1,10)                        bias is always shape of next layer\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Convolution \n",
    "\n",
    "![Figure 1](helper/2.gif)\n",
    "Figure 1 \n",
    "\n",
    "\n",
    "So here we have a depthwise convolution.This operation has to be repeated for n_filters to stack all filters as output.  \n",
    "    for i in range(no_filters):\n",
    "    \n",
    "        image_slice*weights[i]  where image_slice and weights[i] are shape (filter_size*filter_size*no_of_prev_filters)\n",
    "\n",
    "or simply, computing output for all weights together\n",
    "     image_slice * weights \n",
    "     \n",
    "        where image_slice is shape (filter_size*filter_size*no_of_prev_filters), weights (filter_size*filter_size*no_of_prev_filters*no_of_next_filters)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Maxpooling\n",
    "\n",
    "* pooling has no learnable hyperparameter\n",
    "* Number of filters post pooling remain same as previous layer. Just the h,w of output reduces to (h_prev-f)/stride+1\n",
    "* Average pool blurs it by 1 pixel (for filter window 2*2)\n",
    "* Max pooling works better for white object in black bacground, min pooling for black object and white background. \n",
    "\n",
    "\n",
    "    for image_slices in generate_slices:\n",
    "        output[i, j] = np.amax(im_region, axis=(0, 1))   # axis=(0, 1) ensures we have max value for each feature map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Softmax \n",
    "\n",
    "input needs to be flattened if a conv layer. \n",
    "\n",
    "weight shape input_flatten, n_of_output_class \n",
    "\n",
    "bias n_of_output_class \n",
    "\n",
    "totals = np.dot(flattened_input, weights) + bias\n",
    "\n",
    "$$ np.exp(totals)/np.sum(np.exp(totals), axis = 0) $$   \n",
    "numerator is a vector of shape n_of_output_class, denominator is scalar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# BackPropagation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Categorical cross-entropy \n",
    "\n",
    "$$ L = -log(y_c) $$\n",
    "$$ L = 0  $$ if i not c \n",
    "\n",
    "label = np.argmax(y)\n",
    "\n",
    "so, loss = -np.log(out[label])\n",
    "\n",
    "gradient is non zero only for index corresponding to label \n",
    "\n",
    "Since we know only the gradient corresponding to true value will be non zero, we find index corresponding to label by checking non zero gradient. If not this way, getting the index for label might be complicated. \n",
    " Now, imagine the softmax array. \n",
    "\n",
    "\n",
    " $$ \\frac{\\delta out[k]}{\\delta t} = -e^{tc}e^{tk}/S^2   $$ if k!=c \n",
    " $$ \\frac{\\delta out[k]}{\\delta t} = e^{tc}(1-e^{tc})/S^2   $$ if k==c\n",
    "\n",
    " So lets make an array for $e^{tc}/S^2$ and change the index corresponding to label eith (1-e^{tc})\n",
    "\n",
    " $$ \\frac{\\delta t}{\\delta w} = input $$  (this will be the flattened input)\n",
    " $$ \\frac{\\delta t}{\\delta b} = 1 $$\n",
    " $$ \\frac{\\delta t}{\\delta input} = w $$\n",
    "\n",
    " $$ \\frac{\\delta t}{\\delta input} = \\frac{\\delta t}{\\delta input}*\\frac{\\delta t}{\\delta input}*\\frac{\\delta t}{\\delta input} $$\n",
    "\n",
    " d_L_d_t = gradient*d_out_d_t     # gradient is scalar, d_out_d_t is 10 \n",
    "\n",
    "        #d_t_d_w is last_input: 13*13*8, we need (13*13*8, 10)\n",
    "\n",
    "        d_L_d_t = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]   #newaxis, because we need two arrays to do matrix multiplication\n",
    "\n",
    "        d_L_d_w = d_L_d_out*d_out_d_t*d_t_d_w\n",
    "        d_L_d_b = d_L_d_t\n",
    "        d_L_d_inputs = d_t_d_inputs*d_L_d_t   #d_t_d_inputs = weights (13*13*8, 10), d_L_d_t is (10,1) or (10)?    #DOUBT: we need a newaxis here?\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Max Pool \n",
    "Going back to Figure 1, for every pixel in output feature map, there is a corresponding image_slice. \n",
    "So for every dL_dout, we have a corresponding image_slice.\n",
    "Find arg max for each of these image slices. \n",
    "Only update d_L_d_input = dL_dout for the index which has max value.  \n",
    "Repeated for all feature maps. \n",
    "\n",
    "\n",
    "    image[stride*ind_out_h+h_slice_max, stride*ind_out_w + w_slice_max, ind_f] = d_L_d_out[ind_out_h, ind_out_w, ind_f]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Average Pooling \n",
    "\n",
    "$$ avg = \\frac{\\sum x}{n} $$\n",
    "\n",
    "Thus d_avg_d_x = 1/n      (read in quora) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Convolution\n",
    "\n",
    "Changing any weight will change the output.  Every output pixel uses every pixel weight. \n",
    "In forward pass, out[i,j] = image_slice@(stride*i,stride*j)*weights[f] \n",
    "So for d_out_d_w \n",
    " Every out is affected by the correponding image_slice. \n",
    "\n",
    "image_slice@(stride*i,stride*j)  * out[i,j] gives trickle down from one out pixel. We need to sum these for all out pixels. \n",
    "\n",
    "out[i,j,f] is scalar, image_slice@(stride*i,stride*j) is f*f*n_prev_filter \n",
    "We need weight f*f*n_prev_filter*n_filter_out\n",
    "\n",
    "So, we get last dimension by stacking values for each filter as below\n",
    "\n",
    "for generate image_regions:\n",
    "    for f in range(out_filters):\n",
    "        d_L_d_f[f] += image_slice*d_L_d_out[i,j,f]\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
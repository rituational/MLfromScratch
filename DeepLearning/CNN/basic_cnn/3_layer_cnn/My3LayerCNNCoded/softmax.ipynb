{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss = -np.log(p_c[label])   \n",
    "label = np.argmax(y)\n",
    "\n",
    "if y_hat[label] >threshold:\n",
    "    acc = 1 \n",
    "else:\n",
    "    acc = 0\n",
    "\n",
    "\n",
    "dL_dp = -(1/p_c[label]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = np.zeros(10)\n",
    "gradient[label] = -1/out[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = np.exp(z[label])\n",
    "total = np.exp(z)\n",
    "\n",
    "p_c = num/total \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax: \n",
    "\n",
    "    # def __init__:\n",
    "    #     self.W = np.random.random(flattened_input,10)\n",
    "    #     self.b = np.zeros(10)\n",
    "\n",
    "    def forward(input):\n",
    "        self.last_input_shape = input.shape\n",
    "\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "\n",
    "        totals = np.dot(input, self.weights) + self.biases \n",
    "        self.last_totals = totals\n",
    "\n",
    "        exp = np.exp(totals)\n",
    "\n",
    "        return exp/np.sum( exp, axis = 0)\n",
    "\n",
    "\n",
    "    def backward(self, d_L_d_out):\n",
    "        \n",
    "        for i, gradient in enumerate(d_L_d_out):    #NOTE:Read the awesomeness of this implementation below. Else i was lost in the commented version. By checking for non zero gradient, we get the index corresponding to our label. \n",
    "             # for i in range(out.shape[-1]):\n",
    "        #     exp = np.exp(self.totals)\n",
    "        #     exp_totals = np.sum( exp, axis = 0)\n",
    "        #     S = exp/exp_totals\n",
    "        #     if i == label:\n",
    "        #         d_out_d_t = exp[label]*exp[i]/exp_totals**2\n",
    "        #     else: \n",
    "        #         d_out_d_t = exp[label]*(exp_totals-exp[label])/exp_totals**2\n",
    "\n",
    "            if gradient == 0:\n",
    "                continue \n",
    "        \n",
    "        t_exp = np.exp(self.last_totals)\n",
    "        S = np.sum(t_exp, axis = 0)\n",
    "        d_out_d_t = -t_exp[i]*t_exp/(S**2)\n",
    "        d_out_d_t[i] =  t_exp[i]*(S-t_exp[i])/(S**2)\n",
    "\n",
    "        d_t_d_w = self.last_input\n",
    "        d_t_d_b = 1 \n",
    "        d_t_d_inputs = self.weights\n",
    "\n",
    "        d_L_d_t = gradient*d_out_d_t     # gradient is scalar, d_out_d_t is 10 \n",
    "\n",
    "        #d_t_d_w is last_input: 13*13*8, we need (13*13*8, 10)\n",
    "\n",
    "        d_L_d_t = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]   #newaxis, because we need two arrays to do matrix multiplication\n",
    "\n",
    "        d_L_d_w = d_L_d_out*d_out_d_t*d_t_d_w\n",
    "        d_L_d_b = d_L_d_t\n",
    "        d_L_d_inputs = d_t_d_inputs*d_L_d_t   #d_t_d_inputs = weights (13*13*8, 10), d_L_d_t is (10,1) or (10)?    #DOUBT: we need a newaxis here?\n",
    "\n",
    "        self.weights -= learn_rate * d_L_d_w\n",
    "        self.biases -= learn_rate * d_L_d_b\n",
    "\n",
    "      return d_L_d_inputs.reshape(self.last_input_shape) #because previous layer is conv layer"
   ]
  },
  {
   "source": [
    " Since we know only the gradient corresponding to true value will be non zero, we find index corresponding to label by checking non zero gradient. If not this way, getting the index for label might be complicated. \n",
    " Now, imagine the softmax array. \n",
    "\n",
    "\n",
    " $$ \\frac{\\delta out[k]}{\\delta t} = -e^{tc}e^{tk}/S^2   $$ if k!=c \n",
    " $$ \\frac{\\delta out[k]}{\\delta t} = e^{tc}(1-e^{tc})/S^2   $$ if k==c\n",
    "\n",
    " So lets make an array for $e^{tc}/S^2$ and change the index corresponding to label eith (1-e^{tc})\n",
    "\n",
    " $$ \\frac{\\delta t}{\\delta w} = input $$  (this will be the flattened input)\n",
    " $$ \\frac{\\delta t}{\\delta b} = 1 $$\n",
    " $$ \\frac{\\delta t}{\\delta input} = w $$\n",
    "\n",
    " $$ \\frac{\\delta t}{\\delta input} = \\frac{\\delta t}{\\delta input}*\\frac{\\delta t}{\\delta input}*\\frac{\\delta t}{\\delta input} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
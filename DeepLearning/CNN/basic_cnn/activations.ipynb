{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Activation Functions?\n",
    "In Perceptron, Replace step function with the logistic (sigmoid) function, σ(z) = 1 / (1 + exp(–z)). This was essential because the step function contains only flat segments, so there is no gradient to work with (Gradient Descent cannot move on a flat surface), while the logistic function has a well-defined nonzero derivative everywhere, allowing Gradient Descent to make some progress at every step\n",
    "\n",
    "Well, if you chain several linear transformations, all you get is a linear transformation. For example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can’t solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function.\n",
    "\n",
    "## RelU & Variants:\n",
    "\n",
    "### RelU : max(0, z)\n",
    "* Relu is faster than sigmoid which has comparitively complex gradient calculations\n",
    "* Dead ReLus: When weighted sum of inputs is negative, output is zero. No update since grad is zero.\n",
    "(But if it isnt the first layer, during g.d. the updates to input layer may ensure weighted sum of dead neurons is >0)\n",
    "\n",
    "\n",
    "### Softplus : log(1 + exp(z))\n",
    "*More smoother version of RelU (in terms of differentiation)\n",
    "close to 0 when -\n",
    "close to z when +\n",
    "Smoother than RelU\n",
    "\n",
    "![](helper/4.JPG)\n",
    "\n",
    "\n",
    "### Leaky RelU:\n",
    "\n",
    "* max($\\alpha$ z, z)\n",
    "* $\\alpha$ -> determines how much leak it can take ~ 0.01.\n",
    "* Ensures they may go to coma but never die out\n",
    "\n",
    "### RReLU:\n",
    "\n",
    "* Randomized Leaky RelU\n",
    "* $\\alpha$ is picked randomly from given range & fixed to avg during testing\n",
    "\n",
    "\n",
    "### PReLU:\n",
    "\n",
    "Parametric ReLU\n",
    "* $\\alpha$ is learned during training\n",
    "* Better with large datasets, poor with small datasets (overfitting)\n",
    "\n",
    "### ELU:\n",
    "\n",
    "* Exponential Linear Unit. Exp decreasing when -\n",
    "$\\alpha$(exp (z) - 1) if -\n",
    "z if +\n",
    "* Slower compute\n",
    "\n",
    "### SELU:\n",
    "\n",
    "* Scaled ELU\n",
    "* Self -Normalize : if all hidden layers had SELu -> each output layers will have mean 0, stddev 1 -> no vanishing/exloding grad problem\n",
    "Constraints:\n",
    "Inputs to be normalized\n",
    "Weights -> lecun normalization\n",
    "WOrks only with sequential data\n",
    "Summary :\n",
    "\n",
    "SELU > ELU > leak RELU > RELU > Tanh > sigmoid\n",
    "\n",
    "If self-normalization not possible ELu > SELU\n",
    "\n",
    "Run time latency -> LRELU > SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(X):\n",
    "    out = np.maximum(X, 0)\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    dX = dout.copy()\n",
    "    dX[cache <= 0] = 0\n",
    "    return dX\n",
    "\n",
    "\n",
    "def lrelu_forward(X, a=1e-3):\n",
    "    out = np.maximum(a * X, X)\n",
    "    cache = (X, a)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def lrelu_backward(dout, cache):\n",
    "    X, a = cache\n",
    "    dX = dout.copy()\n",
    "    dX[X < 0] *= a\n",
    "    return dX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

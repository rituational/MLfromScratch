{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Dropout \n",
    " \n",
    "Switch off a random fraction of neurons at every iteration. \n",
    "Disadv: during inference, we need to scale by keep probability\n",
    "\n",
    "## Inverted Dropout \n",
    "Switch off a random fraction of neurons at every iteration. Scale by 1/keep_prob in the forward pass itself. This way we dont need to have scaling in inference. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inverted Dropout Implementation\n",
    "\n",
    "dist_for_mask = np.random.rand(*H1.shape) #rand generates uniform distribution \n",
    "# mask applied on the layer, not weights??\n",
    "mask = dist_for_mask < keep_prob # bool mask, only values less than keep_prob are true\n",
    "\n",
    "H1 = H1*mask/keep_prob #elementwise mult. Only keep_prob number of neuros non zero\n",
    "\n",
    "H2 = H1@W1 +b1\n",
    "\n",
    "def back_prop:\n",
    "    d_out_d_H = d_out_d_H*mask\n",
    "\n",
    "def testing:\n",
    "    no scaling required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    cache = u\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dX = dout * cache\n",
    "    return dX"
   ]
  },
  {
   "source": [
    "References:\n",
    "\n",
    "https://wiseodd.github.io/techblog/2016/06/25/dropout/\n",
    "\n",
    "https://github.com/wiseodd/hipsternet/blob/master/hipsternet/layer.py  (good implementation of all cnn layers)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing Gradients: \n",
    "The gradients reduce as it back propogates to lower layers, and lower layers remain virtually unchanged. ( because of poor choice of weight initialization and activation))\n",
    "\n",
    "* For normal distributed weight initialization ($\\mu$ 0 and s.d. 1) and sigmoid, the output variance of a layer was higher than input variance. This chained together as multiple layers and getting saturated with sigmoid layer at the endcaused problem (imagine multiple microphones in series, if each amplifies the output will be saturated and inaudible)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot Intialization\n",
    "\n",
    "\n",
    "Variance of output layer should be equal to the variance of the input.  We need gradients to have equal variance before and after a layer. \n",
    "\n",
    "Fan in = number of inputs \n",
    "Fan out = number of neurons \n",
    "\n",
    "Xavier/Glorot Intialization = normal distribution with $\\sigma = \\frac{1}{\\sqrt{F_{avg}}}$ \n",
    "or uniform distribution with $\\sigma = \\sqrt{\\frac{3}{F_{avg}}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](helper/3.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has default glorot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands On\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5.,  44., 101., 179., 249., 211., 127.,  58.,  19.,   7.]),\n",
       " array([-2.84703103, -2.24260327, -1.63817551, -1.03374776, -0.42932   ,\n",
       "         0.17510776,  0.77953552,  1.38396328,  1.98839103,  2.59281879,\n",
       "         3.19724655]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.normal(0, 1, 1000 )\n",
    "plt.hist(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just driving home the fact that even though std is 1, there are outliers who have magnitude 3 as well. Below, when we have many layers with normal distribution and std 1, these outliers cause the explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n"
     ]
    }
   ],
   "source": [
    "## Forward Prop: Assume no activation\n",
    "\n",
    "x = tf.random.normal(shape =(1,100),mean = 0, stddev = 1)\n",
    "\n",
    "w = tf.random.normal(shape = (100,100), mean = 0, stddev = 1)\n",
    "\n",
    "for _ in range(100):\n",
    "    x = tf.matmul(x, w)\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check when the value became NaN. Looks like our weights were initialized large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal(shape =(1,100),mean = 0, stddev = 1)\n",
    "\n",
    "w = tf.random.normal(shape = (100,100), mean = 0, stddev = 1)\n",
    "\n",
    "for i in range(100):\n",
    "    x = tf.matmul(x, w)\n",
    "    if tf.math.is_nan(tf.reduce_mean(x).numpy()):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we multiply higher magnitude weights, faster exploding happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal(shape =(1,100),mean = 0, stddev = 1)\n",
    "\n",
    "w = tf.random.normal(shape = (100,100), mean = 0, stddev = 1)*10\n",
    "\n",
    "for i in range(100):\n",
    "    x = tf.matmul(x, w)\n",
    "    if tf.math.is_nan(tf.reduce_mean(x).numpy()):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have low magnitude weights, we get vanishing gradients as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.5296789e-30 0.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal(shape =(1,100),mean = 0, stddev = 1)\n",
    "\n",
    "w = tf.random.normal( shape = (100,100), mean = 0, stddev = 1)*0.05\n",
    "\n",
    "for _ in range(100):\n",
    "    x = tf.matmul(x, w)\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the std of the output. Below we see that the output std seems to be related to the square root of the input. \n",
    "### Intuition: \n",
    "We are multiplying 100 distributions with mean 0 and variance 1. So their product will have variance 512, std = $\\sqrt{512}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03723511945083737 22.538990865707397\n",
      "22.627416997969522\n"
     ]
    }
   ],
   "source": [
    "mean, stddev = 0, 0\n",
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "    w = tf.random.normal((512, 512), mean = 0, stddev=1)\n",
    "    x = tf.random.normal((1, 512), mean = 0, stddev=1)\n",
    "    y = tf.matmul(x, w)\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)\n",
    "print(np.sqrt(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001817455030977726 0.9971253992915153\n"
     ]
    }
   ],
   "source": [
    "## Lets check experimentally that diving by sqrt(no_of_inputs) works \n",
    "mean, stddev = 0, 0\n",
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "    w = tf.random.normal((512, 512), mean = 0, stddev=1)/np.sqrt(512)\n",
    "    x = tf.random.normal((1, 512), mean = 0, stddev=1)\n",
    "    y = tf.matmul(x, w)\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21519707 1.3488483\n"
     ]
    }
   ],
   "source": [
    "## Lets chec if it works for our 100 layer network\n",
    "\n",
    "x = tf.random.normal((1, 100), mean = 0, stddev=1)\n",
    "for i in range(100):\n",
    "    w = tf.random.normal((100, 100), mean = 0, stddev=1)*1/np.sqrt(100)\n",
    "    x = tf.matmul(x, w)\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "Lets check what happens when we add activation. Consider symmetrical tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011288065 0.06097671\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((1, 100), mean = 0, stddev=1)\n",
    "for i in range(100):\n",
    "    w = tf.random.normal((100, 100), mean = 0, stddev=1)*1/np.sqrt(100)\n",
    "    x = np.tanh(tf.matmul(x, w))\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The std has reduced a lot. So standard initialization doesnt seem t work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030923216 0.0797521\n"
     ]
    }
   ],
   "source": [
    "def xavier(m, h):\n",
    "    return tf.random.normal((m, h))*np.sqrt(2/(m + h))\n",
    "\n",
    "x = tf.random.normal((1, 512))#, mean = 0, stddev=1)\n",
    "\n",
    "for i in range(100):\n",
    "    w = xavier(512,512)\n",
    "    x = np.tanh(tf.matmul(x, w))\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())\n",
    "\n",
    "## In our example it seems to work pretty similar to the standard initialization. But in the paper they told it works better as per the blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu\n",
    "\n",
    "In case of Relu, output variance is $\\sqrt{\\frac{2}{num\\_inputs}}$\n",
    "(In original blog std came to be 16, mine is coming as 13 dont know why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.032847576141357 13.168011637687684\n",
      "16.0\n"
     ]
    }
   ],
   "source": [
    "mean, stddev = 0, 0\n",
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "    w = tf.random.normal((512, 512), mean = 0, stddev=1)\n",
    "    x = tf.random.normal((1, 512), mean = 0, stddev=1)\n",
    "    y = tf.nn.relu(tf.matmul(x, w))\n",
    "    mean += tf.reduce_mean(y).numpy()\n",
    "    stddev += tf.math.reduce_std(y).numpy()\n",
    "print(mean/num_iter, stddev/num_iter)\n",
    "print(np.sqrt(512/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((1, 512))#, mean = 0, stddev=1)\n",
    "\n",
    "for i in range(100):\n",
    "    w = tf.random.normal((512, 512))\n",
    "    x = tf.nn.relu(tf.matmul(x, w))\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.065543e-16 7.45431e-16\n"
     ]
    }
   ],
   "source": [
    "def xavier(m, h):\n",
    "    return tf.random.normal((m, h))*np.sqrt(2/(m + h))\n",
    "\n",
    "x = tf.random.normal((1, 512))#, mean = 0, stddev=1)\n",
    "\n",
    "for i in range(100):\n",
    "    w = xavier(512,512)\n",
    "    x = tf.nn.relu(tf.matmul(x, w))\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())\n",
    "\n",
    "# This isnt working the way it should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3402927 0.48951837\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((1, 512))#, mean = 0, stddev=1)\n",
    "\n",
    "for i in range(100):\n",
    "    w = tf.random.normal((512, 512))*np.sqrt(2/512)\n",
    "    x = tf.nn.relu(tf.matmul(x, w))\n",
    "print(tf.reduce_mean(x).numpy(), tf.math.reduce_std(x).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He Initialization \n",
    "* Populate weights as normal distribution \n",
    "* Scale by $\\sqrt{\\frac{2}{num\\_inputs}}$\n",
    "* Initialize bias to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnings: \n",
    "* If weight initialized too low, causes gradients to vanish, if too high gradient explodes. \n",
    "* A forward prop can be seen as matrix multiplcation by weights n times, to maintain output variance as 1 (input is normal distribution with variance 1) we need to divide by $\\sqrt{no_of_inputs}$. This works if we have no activation\n",
    "* With activation, even with divide by $\\sqrt{no\\_of\\_inputs}$, output variance is too low. \n",
    "* For tanh (symmetric activations) Xavier works. \n",
    "* For ReLu, $\\sqrt{2/no\\_of\\_inputs}$ (He) works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

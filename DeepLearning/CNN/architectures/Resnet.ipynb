{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, add, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def naive_resnet_block(in_layer, f):\n",
    "    out_layer = Conv2D(f, (3,3), padding = 'same', activation = 'relu')(in_layer)\n",
    "    out_layer = Conv2D(f, (3,3), padding = 'same', activation = 'linear')(out_layer)\n",
    "\n",
    "    ## my first intuition was to concat the out with the input layer. But in resnet, we are adding the skip connection. This means \n",
    "    ## that the number of filters in input layer should be the same as the last conv layer\n",
    "    \n",
    "    ## notice: we add first and then apply activation to the addition. Hence last conv layer has linear activation.\n",
    "    out_layer = add([out_layer, in_layer])\n",
    "    \n",
    "    out_layer = Activation('relu')(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "def residual_block(in_layer, f):\n",
    "    \n",
    "    out_layer = Conv2D(f, (3,3), padding = 'same', activation = 'relu')(in_layer)\n",
    "    out_layer = Conv2D(f, (3,3), padding = 'same', activation = 'linear')(out_layer)\n",
    "    \n",
    "    ## we add a 1*1 convolution stage (referred as projection layer)\n",
    "    \n",
    "    if in_layer.shape[-1]!=f:\n",
    "        in_layer = Conv2D(f, (1,1), padding = 'same', activation = 'relu')(in_layer)\n",
    "     \n",
    "    out_layer = add([out_layer, in_layer])\n",
    "    \n",
    "    out_layer = Activation('relu')(out_layer)\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 256, 64) 1792        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 256, 64) 256         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256, 256, 64) 0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 256, 64) 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 256, 32) 18464       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 256, 256, 32) 2080        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256, 256, 32) 0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 256, 256, 32) 0           add_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 68,768\n",
      "Trainable params: 68,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "visible = Input(shape=(256, 256, 3))\n",
    "# add inception block 1\n",
    "layer = residual_block(visible, 64)\n",
    "# add inception block 1\n",
    "layer = residual_block(layer, 32)\n",
    "# create model\n",
    "model = Model(inputs=visible, outputs=layer)\n",
    "# summarize model\n",
    "model.summary()\n",
    "# plot model architecture\n",
    "plot_model(model, show_shapes=True, to_file='module.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/helper/inception.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](helper/resnet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings \n",
    "\n",
    "* my first intuition was to concat the out with the input layer. But in resnet, we are adding the skip connection. This means \n",
    "  that the number of filters in input layer should be the same as the last conv layer\n",
    "    \n",
    "* notice: we add first and then apply activation to the addition. Hence last conv layer has linear activation.\n",
    "    \n",
    " * we add a 1*1 convolution stage (referred as projection layer) to the input layer to make no of filters equal to last conv layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

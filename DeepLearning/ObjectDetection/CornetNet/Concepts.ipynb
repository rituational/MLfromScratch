{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![](helper/1.JPG)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We use hour glass blocks to learn th esemantic representations \r\n",
    "\r\n",
    "\r\n",
    "* We predict the top left and bottom right corner for each object. \r\n",
    "* We have two different networks for each \r\n",
    "* We have embeddings to match which top left point corresponds to which bottom right (eg. In case of multiple people close by) We train the ebedding to have less distance for same object, more for different objects \r\n",
    "\r\n",
    "\r\n",
    "* While training, we will have a heatmap for both corners. Ideally we have 1 at corner, zero at the rest, but to give some margin, we add 0.9 around the center.\r\n",
    "* Each heatmap will be H*W*C (no. of classes)\r\n",
    "* For embeddings, we try to minimize the sum of squares of corner ebeddings with mean (top +bottom)/2 \r\n",
    "* We maximize the mean embedding of one class to all other classes.   \r\n",
    "![](helper/2.JPG)\r\n",
    "* Embedding is just n=1, so its a number \r\n",
    "* *Corner pooling*: To make sure we have high level information rather than just looking near the corner, we take a max pool for the whole patch of row and column \r\n",
    "![](helper/3.JPG)\r\n",
    "\r\n",
    "* Offsets: When we downsample, we lose location information because of rounding, offsets compensate for that (didnt really go into that details)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](helper/4.JPG)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "SIMD: Single instruction multiple data \n",
    "\n",
    "\n",
    "Kernel level optimization \n",
    "* Operator simplification \n",
    "- shift the sqrt to constant value rather than variable (val = sqrt(var); val > const â†’ var>const^2)\n",
    "- Horner's rule: pull out the commmon factor from polynomial expression\n",
    "* Calculations Reuse Across Iterations (eg. convolution with stride 1, the calculations from [2,n-1] can eb reused)\n",
    "* Calculation reuse across iterations\n",
    "* avoid control code, write code in linear fashion. Not nested for loops\n",
    "* Fixed point, Q point, so the bits are assigned as  8 integers and 7 fractions\n",
    "* lookup tables instead of sin, sqrt etc. Quantize float and use LUT \n",
    "* Array of Structures (AOS) vs Structure of Arrays (SOA)\n",
    "\n",
    "Loop Transformations \n",
    "* Collapsing smaller inner looop, loop fusion, fission\n",
    "* Load balancing \n",
    "\n",
    "\n",
    "Ref: \n",
    "\n",
    "https://www.ti.com/lit/an/spna165/spna165.pdf?ts=1626326802278&ref_url=https%253A%252F%252Fwww.google.com%252F"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Dynamic range is the ratio between the largest and smallest values that a certain quantity can assume"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Terms:\n",
    "* Latency, bandwidth, throughput, dynamix range,\n",
    "* Different scaling for weights, bias (knowledge of dynamic range of alll layers from training)\n",
    "* per channel scaling: helps provide flexibility in quantization\n",
    "* Scale only quantization (centered quantization, no offset ) unscaled: you can use unsigned bit and save one bit that doesnt give any imkprovement\n",
    "* FP16 quantization, int8, mixed precision (in yolo v1, deeper layers should be higher precision as more susceptible to noise )\n",
    "* Match the output distribution bertween fp32, int8 checking the KL divergence\n",
    "\n",
    "\n",
    "\n",
    "* Train for quantization (forward pass clip weight to nearest quantized value. Backprop dont clip. Activations unquantized )\n",
    "\n",
    "\n",
    "Ref: \n",
    "https://www.youtube.com/watch?v=VsGX9kFXjbs&t=2159s\n",
    "\n",
    "![](helper/8.JPG)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Nvidia Yolov1 tensroRT example \n",
    "\n",
    "Best from Int8: \n",
    "* setDynamicRange \n",
    "* mixed precision:\n",
    "**type constraint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Covers all quantization \n",
    "https://arxiv.org/pdf/2103.13630.pdf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Quantization aware training \n",
    "\n",
    "* Train for quantization (forward pass clip weight to nearest quantized value. Backprop dont clip. Activations unquantized )\n",
    "\n",
    "![](helper/4.JPG)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ref : https://www.youtube.com/watch?v=-jBmqY_aFwE \n",
    "\n",
    "* Since the data to DNN is kind of noisy and quantization also adds noise, it doestn affect the accuracy number that much. But if the quantization introduces a bias, shifts the distribution it can be detrimental. Until it adds uniform noise thats fine. \n",
    "* Since we have a lot of sparsity it is very important to encode 0.0 correctly.\n",
    "\n",
    "* Quantized inference calculations: We have converted weights into 8 bits, we also convert input to 8 bit (imagine a step functiohn, we floor/ceil to nearest step)\n",
    "* DSP dont support floating points, but with int8 they can be used\n",
    "\n",
    "* Symmetric vs asymmetric: When we quantized weights, we were just converting them into an 8 bit and converting them back to FP later. But here we need to do arithematic operations, thus we need to ensure that: \n",
    "\n",
    "** zero is always zero\n",
    "** encoded (-x) should also flip the sign \n",
    "\n",
    "So we stored it in signed format with 0 exactly at 0. But this causes the following problems: \n",
    "* We have one less bit for the negative range (-127,0) than positive (0,128) which can cause problems\n",
    "Also, earlier we coulod convert -4.5 to 5.4 to 255 bits. Now even though min value is -4.5 we need to have a range of (-5.4 to 5.4) (as 0 has to be in the middle)\n",
    "\n",
    "\n",
    "How to find this range (a.k.a dynamic range??): \n",
    "* Analyze largest possible range (check max input and min input and calculate what dot product min max will be )\n",
    "* Training logging\n",
    "* Post training calibration\n",
    "\n",
    "\n",
    "We have two 8 bit beign multiply which gives 16 bits and then dot product adds many of them, so we have to kee pthe output of the dot product as 32 bit, but again not the whole range may be used. We can use 16 bit accumulaters to keep track of it (no very clear) \n",
    "\n",
    "* Per channel scaling\n",
    "output_n = input_n*weight_n \n",
    "So now weight_n is scaled differently for each channel and we scale  back the output accordingly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ref: https://www.youtube.com/watch?v=AgezOkBTV90\n",
    "\n",
    "![](helper/9.JPG)\n",
    "![](helper/10.JPG)\n",
    "![](helper/11.JPG)\n",
    "Then do huffman encoding\n",
    "\n",
    "\n",
    "\n",
    "Knowledge Distillation: Student teacher network\n",
    "\n",
    "* Lottery Ticket Hypothesis: Often a small subset of NN can perform equivalent. So how to fidn that? Randomly prune p% of the smallest weights. If it works thats your network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### M E \n",
    "\n",
    "* software pipelining:linear assemblers \n",
    "* Redundancy in code: reuse loaded data\n",
    "* Code partitioning: cluster based DSP "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
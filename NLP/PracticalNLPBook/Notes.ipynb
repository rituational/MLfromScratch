{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Techniques \n",
    "* Naive Bayes: Assumes that each feature is independent of other feature    \n",
    "* SVM: Handles noise well but slow on training and scaling is issue \n",
    "* HMM:\n",
    "* * Each hidden state is dependent on previous states. \n",
    "* * Utilizes the sequential nature of language\n",
    "\n",
    "![](helper/1.JPG)\n",
    "* Conditional Random Fields (CRF): \n",
    "* * Performs classification task for each element in a sequence. \n",
    "* * Outperforms others on Part of Speech tagging. \n",
    "\n",
    "## DL Techniques\n",
    "* RNN \n",
    "* LSTM, GRU \n",
    "* CNN\n",
    "* Transformers, self attention \n",
    "* Autoencoders \n",
    "\n",
    "## Why DL isnt yet a silver bullet \n",
    "* Overfitting on small dataset \n",
    "* Lack of Domain generalization\n",
    "* DOesnt yet grasp common sense knowledge, logic\n",
    "* Costly \n",
    "* Less Interpretable \n",
    "* Less avenues for few shot learning and data augmentation \n",
    "* Difficult to deploy on edge \n",
    "* Higher technical Debt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we typically lowercase the text before stemming. We also donâ€™t remove tokens or lowercase the text before doing lemmatization because we have to know the part of speech of the word to get its lemma, and that requires all tokens in the sentence to be intact. A good practice to follow is to prepare a sequential list of pre-processing tasks to be done after having a clear understanding of how to process our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline \n",
    "DO READ IT AGAIN. Quite Comprehensive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation \n",
    "* How to convert text into numbers. \n",
    "* It isnt as stright forward as images, speech where the input is inherently represented as numbers.\n",
    "* * Basic vectorization approaches\n",
    "* * Distributed representations\n",
    "* * Universal language representation\n",
    "* * Handcrafted features\n",
    "\n",
    "## Vectorization approaches\n",
    "### One Hot Encoding \n",
    "* Make a corpus of all the words, mapping each word to a unique ID. \n",
    "* Every word is represented as a binary vector of size of corpus, vecotr[unique ID] = 1. We have one such vector for each word in a sentence. \n",
    "* Problems: \n",
    "* * Size if one hot vector is directly proportional to size of corpus: sparsity\n",
    "* * The vector dimension varies with length of sentence, we would prefer fixed size.    \n",
    "* * Words considered atomic units and no dis/similarity between them considered.\n",
    "* * Out of vocab problem. We may need to retrain the model again with new word added. \n",
    "* * Seldom used \n",
    "\n",
    "[code](./TextRepresentation/OneHot.ipynb)\n",
    "### Bag of Words\n",
    "* We can make a corpus and keep a count of occurence each word in a document. \n",
    "* We can also just keep a note of what words occurred sans counts (used in sentiment analysis) \n",
    "* Advantages: \n",
    "* * Easy to interpret and implement \n",
    "* * Documents having same words will be closer in euclidean space, thus considers semantic similarity. \n",
    "* * Fixed length encoding.\n",
    "* Disadvantages: \n",
    "* * Size of vector increases with size of vocab. \n",
    "* * Doesnt capture similarity between words. \n",
    "* * Out of vocab problem. \n",
    "* * Word order is lost. \n",
    "\n",
    "### Bag of N words \n",
    "* It breaks the text into chunks of n contiguous words/ tokens \n",
    "* Adv/ disadv:\n",
    "* * Thus it captures some context, thus can capture semantic similarity to a level \n",
    "* * As 'n' increases, the dimentionality increases rapidly. \n",
    "* * Has Oout of vocab problem \n",
    "\n",
    "### TF IDF (term frequency inverse )\n",
    "* Quantify importance of given word relative to other words in the document. \n",
    "* Used for information retrieval \n",
    "* If word w occurs many times in one particular document but not often in rest of the document, that word is important for that document.\n",
    "* Term frequency:\n",
    "* * How often a term occurs in a doc \n",
    "* * Larger doc may have higher count, hence we normalize term count by length of the document. \n",
    "$$ TF = \\frac{Number of occurence of term t in doc}{total number of terms in doc d} $$ \n",
    "\n",
    "* Inverse document frequency: \n",
    "* * TF gives equal importance to all terms.\n",
    "* * weigh down very common terms (stop words) and weigh up rare occurance.\n",
    "* * $$ IDF = log_e \\frac{total number of documents in corpus}{total number of docs with term t in it} $$ \n",
    "\n",
    "\n",
    "$$ TF-IDF score = TF*IDF $$\n",
    "\n",
    "* For corpus:\n",
    "* S1 = 'dog bites man'\n",
    "* S2 = 'man bites dog'\n",
    "* S3 = 'dog eats meat'\n",
    "* S4 = 'man eats food'\n",
    "\n",
    "<img src=\"helper/2.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "\n",
    "## Distributed representations\n",
    "* Distributional Similarity: Meaning of the word can be understood from the context they appear\n",
    "* Distributional Hypothesis: Words that occur in similar context have similar meaning. \n",
    "* Distributional Representation: High dimensional representation based on occurence of word and context. eg TF IDF, one hot\n",
    "* Distributed Representation: Compress the distributional representations to get compact and dense vectors. \n",
    "\n",
    "### Word Embeddings\n",
    "##### Pretrained \n",
    "* Smaller, denser vectors. \n",
    "* Derives meaning of the word from the context.\n",
    "* Projects words to a vector space where similar words cluster together.\n",
    "\n",
    "* Pretrained word embddings: \n",
    "* * Word2Vec, GloVe, fasttext\n",
    "* * Better to use this and if it doesnt work train custo embedding. \n",
    "\n",
    "#### Disadv: \n",
    "* Cant distinguish homophones: dog bark vs tree bark \n",
    "* memory intensive \n",
    "* Unintentional bias\n",
    "* corpus dependent\n",
    "\n",
    "#### Train Embedding \n",
    "#### CBOW\n",
    "* Continuous Bag of words:predict centre word given context. If we take contextk, we have 2k context words and we predict 1 output prediction: the middle word. \n",
    "* Salient points in diagram below: \n",
    "* * Input will be vector rep (one hot vector) with dimension V. \n",
    "* * Weights V*N (N is length of our embedding == hidden layer). \n",
    "* * Output: one word rep using one hot vector.\n",
    "\n",
    "<img src=\"helper/3.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "<img src=\"helper/4.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "#### Skipgram\n",
    "* predict context words from centre word.\n",
    "* We take a 2k+1 window, from the centre word we predict 2k words. So it gives 2k word pairs.\n",
    "* We shift the window across the whole corpus to geberate training data. \n",
    "* Salient points in diagram below: \n",
    "* * We start with middle word one hot vector.\n",
    "* * A V*N weight converts it to the embedding. \n",
    "* * We have seperate N*V weight to generate each context. \n",
    "\n",
    "<img src=\"helper/5.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "<img src=\"helper/6.JPG\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "Some important hyperparameters: \n",
    "* Dimensionality of word vectors. (50-500)\n",
    "* Context window\n",
    "\n",
    "\n",
    "#### Going beyond words\n",
    "We can find embeddings of constituent words and take sum/avg etc. Though we loose the ordering information, it works suprisingly well. \n",
    "\n",
    "Both self and pre trained embeddings heavily depend on the vocabulary. Vocabulary overlap is a great metric to guage performance of NLP model.\n",
    "* So out of vocaulary can be dealt by removing OOV words during pre processing only. \n",
    "* Or we specificy random embeddings of OOV, this boosts performance by 1-2% rather than just excluding it. \n",
    "* Use subword properties. fastext has embedding for word and characters together, word's embedding is aggregation of charater n grams. \n",
    "* Distributed Representation: \n",
    "* * Even with fasttext n grams, dog bites man and man bites dog will have the same embedding which isnt right. \n",
    "* * Doc2Vec takes arb length text and in addition to word vectors also has 'paragraph vector' (learns words with context)\n",
    "* * While training on large corpus, paragraph vectors are unique to the text (arb length) in consideration while word vectors are shared through across all texts\n",
    "\n",
    "#### Universal Text representation\n",
    "* Contextual word representations: Primitive, predict next word using n gram prev words \n",
    "* Transformers can be used as pre trained models to get text representations. \n",
    "\n",
    "### Things to remember\n",
    "* Any text representation has inherent bias based on the text it was trained on. Eg. embedding trained on tech news will think apple is closer to microsoft vs peers, oranges. \n",
    "* Embeddings are bulky. Word2Vec is 4.5GB: This is can be a bottleneck in deployment. Hack: Use in memory database like Redis with a cache.\n",
    "* We need more than embeddings. Eg: for sarcasm detection. \n",
    "\n",
    "\n",
    "## Visualization\n",
    "\n",
    "### t SNE: t distributed stocastic neighbouring embedding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJWuSxRlepKywC/CQkPgYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rituational/MLfromScratch/blob/master/NLP/PracticalNLPBook/Classification/Doc2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOVPlVFY4CG7",
        "outputId": "0238771c-e495-441e-da70-d92f8b91c671"
      },
      "source": [
        "!pip install nltk==3.5\n",
        "!pip install pandas==1.1.5\n",
        "!pip install gensim==3.8.3\n",
        "!pip install scikit-learn==0.21.3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5) (4.62.3)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434692 sha256=a23863f9a5dfda78525098863ba43eb071f5ca1b25af633feba92cfc699c566b\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 78.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.15.0)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n",
            "Collecting scikit-learn==0.21.3\n",
            "  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3) (1.4.1)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.21.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIhu8CbK4KqO",
        "outputId": "d56057ce-e62e-4286-8c56-1f22a1e104f7"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhHtthzV4UIB",
        "outputId": "7b947974-76b5-47ba-e58c-1864e1f0fefa"
      },
      "source": [
        "#Load the dataset and explore.\n",
        "try:\n",
        "    from google.colab import files\n",
        "    !wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
        "    !wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
        "    !ls -lah DATAPATH\n",
        "    filepath = \"DATAPATH/train_data.csv\"\n",
        "except ModuleNotFoundError:\n",
        "    filepath = \"Data/Sentiment and Emotion in Text/train_data.csv\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-23 17:38:59--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2479133 (2.4M) [text/plain]\n",
            "Saving to: ‘DATAPATH/train_data.csv’\n",
            "\n",
            "train_data.csv      100%[===================>]   2.36M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-11-23 17:39:00 (53.5 MB/s) - ‘DATAPATH/train_data.csv’ saved [2479133/2479133]\n",
            "\n",
            "--2021-11-23 17:39:00--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 783640 (765K) [text/plain]\n",
            "Saving to: ‘DATAPATH/test_data.csv’\n",
            "\n",
            "test_data.csv       100%[===================>] 765.27K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-11-23 17:39:01 (21.9 MB/s) - ‘DATAPATH/test_data.csv’ saved [783640/783640]\n",
            "\n",
            "total 3.2M\n",
            "drwxr-xr-x 2 root root 4.0K Nov 23 17:39 .\n",
            "drwxr-xr-x 1 root root 4.0K Nov 23 17:39 ..\n",
            "-rw-r--r-- 1 root root 766K Nov 23 17:39 test_data.csv\n",
            "-rw-r--r-- 1 root root 2.4M Nov 23 17:39 train_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "rhgq_zEA4Y5R",
        "outputId": "37032522-2104-4af2-a8ed-8dafb9c88301"
      },
      "source": [
        "df = pd.read_csv(filepath)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
              "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2     sadness                Funeral ceremony...gloomy friday...\n",
              "3  enthusiasm               wants to hang out with friends SOON!\n",
              "4     neutral  @dannycastillo We want to trade with someone w..."
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUt_0WD64Ua6",
        "outputId": "990f1051-cc4b-4699-d1d0-5201d80458b1"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "worry         7433\n",
              "neutral       6340\n",
              "sadness       4828\n",
              "happiness     2986\n",
              "love          2068\n",
              "surprise      1613\n",
              "hate          1187\n",
              "fun           1088\n",
              "relief        1021\n",
              "empty          659\n",
              "enthusiasm     522\n",
              "boredom        157\n",
              "anger           98\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHHTbcGZ4bL7",
        "outputId": "bbea3c27-5cc0-4cc2-eb06-4f628d177b94"
      },
      "source": [
        "#Let us take the top 3 categories and leave out the rest.\n",
        "shortlist = ['neutral', \"happiness\", \"worry\"]\n",
        "df_subset = df[df['sentiment'].isin(shortlist)]\n",
        "df_subset.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16759, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU82SBsG4ilz"
      },
      "source": [
        "### Text pre-processing:\n",
        "Tweets are different. Somethings to consider:\n",
        "\n",
        "* Removing @mentions, and urls perhaps?\n",
        "* using NLTK Tweet tokenizer instead of a regular one\n",
        "* stopwords, numbers as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fbGahjX4eIM",
        "outputId": "9de478af-0180-4af5-d52e-a0907852eb81"
      },
      "source": [
        "#strip_handles removes personal information such as twitter handles, which don't\n",
        "#contribute to emotion in the tweet. preserve_case=False converts everything to lowercase.\n",
        "tweeter = TweetTokenizer(strip_handles=True,preserve_case=False)\n",
        "mystopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "#Function to tokenize tweets, remove stopwords and numbers. \n",
        "#Keeping punctuations and emoticon symbols could be relevant for this task!\n",
        "def preprocess_corpus(texts):\n",
        "    def remove_stops_digits(tokens):\n",
        "        #Nested function that removes stopwords and digits from a list of tokens\n",
        "        return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n",
        "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
        "    return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]\n",
        "\n",
        "#df_subset contains only the three categories we chose. \n",
        "mydata = preprocess_corpus(df_subset['content'])\n",
        "mycats = df_subset['sentiment']\n",
        "print(len(mydata), len(mycats))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16759 16759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAaRHpj35A_n",
        "outputId": "d552d0dd-e245-4b47-845b-4a9ece7d11a9"
      },
      "source": [
        "#Split data into train and test, following the usual process\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(mydata,mycats,random_state=1234)\n",
        "\n",
        "#prepare training data in doc2vec format:\n",
        "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(train_data)]\n",
        "#Train a doc2vec model to learn tweet representations. Use only training data!!\n",
        "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm =1, epochs=100)\n",
        "model.build_vocab(train_doc2vec)\n",
        "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfzJS9u25kpe",
        "outputId": "94cd1cae-8d6c-4271-e9f4-55437df405ac"
      },
      "source": [
        "#Infer the feature representation for training and test data using the trained model\n",
        "model= Doc2Vec.load(\"d2v.model\")\n",
        "#infer in multiple steps to get a stable representation. \n",
        "train_vectors =  [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in train_data]\n",
        "test_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in test_data]\n",
        "\n",
        "#Use any regular classifier like logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "myclass = LogisticRegression(class_weight=\"balanced\") #because classes are not balanced. \n",
        "myclass.fit(train_vectors, train_cats)\n",
        "\n",
        "preds = myclass.predict(test_vectors)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(test_cats, preds))\n",
        "\n",
        "#print(confusion_matrix(test_cats,preds))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   happiness       0.38      0.45      0.41       713\n",
            "     neutral       0.47      0.55      0.51      1595\n",
            "       worry       0.58      0.45      0.51      1882\n",
            "\n",
            "    accuracy                           0.49      4190\n",
            "   macro avg       0.48      0.48      0.48      4190\n",
            "weighted avg       0.50      0.49      0.49      4190\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44cwZ4N96BBO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
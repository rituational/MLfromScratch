{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression \n",
    "\n",
    "###  Linearity \n",
    "* y is linearly related to x \n",
    "### Homoscadascity\n",
    "* Variance of residuals is same for any value of x \n",
    "* $y^{\\cap} = w_{0} +w_{1}x + \\epsilon $ \n",
    "$\\epsilon $ is equally likely to be above or below the predicted line \n",
    "![](./../helper/1.JPG) \n",
    "![](./../helper/2.JPG) \n",
    "### Independence \n",
    "* Observations are independent of each other. \n",
    "eg. We cant use LR on time series because each observation may depend on the past values\n",
    "### Normality \n",
    "* For any fixed value of x, y is normal distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving LR\n",
    "\n",
    "#### Normal/Closed Form Solution\n",
    "$$ \\theta = (X^{T}X)^{-1}X^{T}y $$ \n",
    "The inverse will exist if no. of independent observations > no. of features \n",
    "\n",
    "Adv:\n",
    "* No hyper parameter tuning required.\n",
    "\n",
    "Disadv:\n",
    "Computational Complexity:\n",
    "* $(X^T X)^{-1}$ has Computational Complexity of $O(m^{2.4})$ to $O(m^3)$. It increases with increase in number of features.\n",
    "For ex: if you double the number of features, the time increases by $2^{2.4}$ to  $2^{3}$\n",
    "* For pseudo inverse, SVD has Complexity of $O(m^2)$\n",
    " \n",
    "#### Gradient Descent\n",
    "$$ while ||RSS|| <\\epsilon: $$\n",
    "$$ \\theta = \\theta + \\frac{\\alpha}{m}(y-\\hat y)X^{T} $$\n",
    "\n",
    "eg. price vs no. of rooms. We initially start with a small weight that underpredicts, so $y-\\hat y$ is positive on average. We weight it by the feature (intuition: more the weight, more we want it to impact our iteration)\n",
    "Adv: \n",
    "* Well suited when we have large number od observations, features.\n",
    "Disadv: \n",
    "* Tune learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubt \n",
    "In ideal case, if we have only two features, two observations enough to fit a line? Having more observations is useless? Can it cause overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

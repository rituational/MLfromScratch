{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Different Measures of error\n",
    "1. Training error :             \n",
    "Loss in training set. \n",
    "\n",
    "A model with very small training error might not be a good represntation unless training data includes everything your entire population. \n",
    "eg. We fit a high degree polynomial for house prediction, there are lot many up and downs in our predicted value. These ups and downs may not physically make sense, so not a good model, yet training error is low. \n",
    "\n",
    "\n",
    "2. Generalization error :           \n",
    "What we really want. Ideal out of model error when we have the entire population data. But we dont have all of them.        \n",
    "As shown in fig below, at each X output rather than being a point is a distribution of different values it can take (which we might not know) \n",
    "\n",
    "So ideal true relation is shown as red line, best case mean expectation covering all along X mean values.\n",
    "\n",
    "![title](Images\\Generalization.PNG)\n",
    "3. Testing error:       \n",
    "What we can actually compute.         \n",
    "    \n",
    "\n",
    "## 3 Sources of error\n",
    "1. Bias (reducible): \n",
    "\n",
    "low complexity -> high bias\n",
    "Flexibility of our model to capture the true relationship. \n",
    "\n",
    "\n",
    "In example on the right, We took a constant line. We can have multiple such lines for multiple sub samples from our population. Lets take the mean line. We find that the mean line cannot capture our true relation.  \n",
    "\n",
    "\n",
    "Bias = The difference between the avg value of model to the true functional model.\n",
    "\n",
    "$$ Bias \\hat f(x) = E[ \\hat f(x)] - f(x) $$\n",
    "![title](Images\\Bias.PNG)\n",
    "![title](Images\\Bias_Variance.PNG)    \n",
    "\n",
    "2. Variance (reducible) \n",
    "\n",
    "Low complexity model -> low variance\n",
    "\n",
    "\n",
    "How sensitive is the model to the data samples considered. \n",
    "\n",
    "How much do specific fits vary from expected fits \n",
    "\n",
    "\n",
    "In ex below, if we take avg of all these sub-sample complex models, the avg model looks so good at capturing true relationship(low bias), but at extent of high variance. bcos you can see at every x's the amount of variation with change of data is huge.    \n",
    "The variance in itself, all the models built. How much the different f(x) (trained on diff samples) were from each other\n",
    "$$ Variance \\hat f(x) = E[ \\hat f(x) - E[\\hat f(x)])^2] $$        \n",
    "![title](Images\\Variance.PNG)\n",
    "\n",
    "3. Noise (irreducible)          \n",
    "Data inherently noisy. At specific x, the noise can vary, that is referred as variance in noise. No control bcos it doesnt change with better model.        \n",
    "![title](Images\\irreducibleError.PNG)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

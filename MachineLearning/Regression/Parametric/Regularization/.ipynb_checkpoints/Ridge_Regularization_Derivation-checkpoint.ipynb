{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n",
    "\n",
    "$$ Cost = RSS(w) + \\lambda ||W||^2_2 $$\n",
    "$$ Cost = \\sum (y - wX)^2 + \\lambda ||W||^2_2 $$\n",
    "$$ Cost = (y - wX)^T(y - wX) + \\lambda W^T W $$\n",
    "\n",
    "\n",
    "### Case 1 : Closed form Solution \n",
    "Assumptions : \n",
    "In OLS:\n",
    "1. X matrix invertible (N > m)              \n",
    "2. Complexity is less (there arent huge Ns)\n",
    "In Ridge:           \n",
    "1. Can work with N < m as well provided $\\lambda$ > 0.\n",
    "\n",
    "Take gradient and we get,       \n",
    "$$ \\nabla{Cost} = -2X^T(y - wX) + \\lambda 2W $$\n",
    "$$ \\nabla{Cost} = -2X^T(y - wX) + \\lambda 2IW $$\n",
    "It is roughly analagous to derivative of $w^2$ is 2w. I is the identiity matrix(enbled to make things easier. It doesnt alter the calculation though). Solve the above equation to zero to obtain w.\n",
    "$$ \\nabla{Cost} = (X^TX + \\lambda I) W - X^Ty = 0 $$\n",
    "$$ w = (X^TX + \\lambda I)^{-1} X^Ty $$\n",
    "\n",
    "In above equation:\n",
    "$\\lambda$ = 0 : we get w as OLS estimate $ (X^TX)^{-1} X^Ty $             \n",
    "$\\lambda$ = $\\inf$ : we get w = 0, as it is like dividing by $\\inf$\n",
    "\n",
    "Benefit of Ridge: \n",
    "Earlier X^TX was invertible only when we had no. of observations> no. of features. Now, ridge has added $\\lambda$ at every diagonal term. So, the matrix is always invertible unitl $\\lambda$ != 0. Thus, it has regularized our solution in a way.\n",
    "![](./../helper/R2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 : Gradient Descent\n",
    "\n",
    "$$ w = w - \\alpha * \\nabla Cost $$\n",
    "$$ w = w - \\alpha * 2[(X^TX + \\lambda I) W - X^Ty] $$\n",
    "$$ w = (1 - 2\\alpha \\lambda)w + 2\\alpha *[(y- Xw)X^T] $$\n",
    "\n",
    "\n",
    "As shown below, at every step we first reduce our weight to $(1 - 2\\alpha \\lambda)w (\\alpha >=0 \\lambda >=0 and 2\\alpha \\lambda<1) $\n",
    "\n",
    "\n",
    "![](./helper/MR1.JPG)\n",
    "![](./helper/MR2.JPG)\n",
    "\n",
    "Then we add add our RSS component (like usual least squares). This way, at every iteration, we make sure to reduce model complexity by reducing magnitude of weights. \n",
    "\n",
    "\n",
    "\n",
    "#### Note \n",
    "We may need to ensure 2\\alpha \\lambda<1, else it may make our GD unstable, which i saw experimentally. (weight jumping from positive to negative?\n",
    "\n",
    "![](./../helper/R3.JPG)\n",
    "[Code for above graph](./Ridge_GD_Hands_On.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Ridge:\n",
    "1. It can handle lots of features (N < M)\n",
    "2. Can handle bias variance trade-off automatically         \n",
    "3. Can obtain best $\\lambda$ through CV         \n",
    "4. As $\\lambda$ increases, the coeff go to zero         \n",
    "5. Two ways for ridge : Closed form, GD\n",
    "6. Closed form always exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

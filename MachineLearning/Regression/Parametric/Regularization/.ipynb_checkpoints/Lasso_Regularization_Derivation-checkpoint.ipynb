{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of Lasso\n",
    "#### Theory:    \n",
    "$$ Cost = RSS(w) + \\lambda |w|_1 $$\n",
    "1. No-closed form solution for Lasso. The derivative of $|w|_1$, absolute terms is not straight forward(derivative of -ve terms is -1, +ve terms is +1 & at 0 there doesnt exist derivative).           \n",
    "3. Co-ordinate Descent : Instead of computing gradients for all features together, we minimize cost function one coefficient at a time and keeps others fixed. Then we moves to other features and follow same method recursively. \n",
    "\n",
    "![](helper/C1.JPG) \n",
    "\n",
    "4. As per co-ordinate descent, we evaluate the  w's through $\\rho$ whre \\rho$ are the predictions excluding w of one particular feature. If they are high implying high correlation. And we're gonna to set w hat j large, we're gonna put a weight strongly on this feature being in the model. But in contrast if they're uncorrelated, if they don't tend to agree across observations, or if the residuals themselves are already very small, because the predictions are already very good. Then what it's saying is that row j is gonna be small and as a result, we're not gonna put much weight on this feature in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac {\\delta RSS}{\\delta w_{j}} = -2\\sum \\limits _{i = 0} ^{N} h_{j}(x)(y_{i}- \\sum \\limits _{k = 0} ^{D}w_{k}h_{k}x) $$\n",
    "\n",
    "where D = no. of features, N = no. of observation \n",
    "\n",
    "\n",
    "Note: $ y_{i}- \\sum \\limits _{k = 0} ^{D}w_{k}h_{k}x $  We need residual for all observations, hence we sum over i = 0 to N. For each observation our y_pred is $ \\sum \\limits _{k = 0} ^{D}w_{k}h_{k}x  $ so we summate over number of features. For $j^{th}$ weight we multiply this sum of residuals by weight itself. So we have only $ h_{j}(x) $ for gradient of $ w_{j} $ \n",
    "\n",
    "\n",
    "\n",
    "Keeping everything constant apart from $ w_{j} $ we can separate out constant vs variable as below\n",
    "$$ \\frac {\\delta RSS}{\\delta w_{j}} = -2\\sum \\limits _{i =0} ^{N} h_{j}(x)(y_{i}- \\sum \\limits _{k != j} w_{k}h_{k}x -  w_{j}h_{j}x)$$\n",
    "$$ \\frac {\\delta RSS}{\\delta w_{j}} = -2\\sum \\limits _{i =0} ^{N} h_{j}(x)(y_{i}- \\sum \\limits _{k != j} w_{k}h_{k}x) +2 w_{j}\\sum \\limits _{i =0} ^{N} (h_{j}(x))^2$$\n",
    "\n",
    "If we have normalize the data, we have  $ (h_{j}(x))^2 = 1 $\n",
    "\n",
    "lets take \n",
    "$$ \\rho = -2\\sum \\limits _{i =0} ^{N} h_{j}(x)(y_{i}- \\sum \\limits _{k != j} w_{k}h_{k}x)  $$\n",
    "\n",
    "$$ \\frac {\\delta RSS}{\\delta w_{j}} = -2\\rho +2w_{j} = 0 $$\n",
    "\n",
    "\n",
    "\n",
    "$$ w_{j} = \\rho $$ \n",
    "\n",
    "So we check the residuals without $ j^{th} $ term. If our residual is large, that means this feature is important hence we assing that large value to weight. Again we multiply by the feature value because we want our update to be proportional to that\n",
    "\n",
    "### Adding Lasso term \n",
    "Since we dont have differentiability at x = 0, we use subgradients. In lay man terms, our gradient is -1 for x<0, 1 for x >0. So we say the subgradient is any line which lower bounds the function. So any line with slope [-1,1] will be our subgradient at x = 0\n",
    "\n",
    "$$ \\frac {\\delta RSS}{\\delta w_{j}} = -2\\rho +2w_{j} + \\lambda subgradient(|x|) $$\n",
    "\n",
    "#### Case 1: when  $w_{j}$<0\n",
    "\n",
    "$$ \\frac {\\delta RSS}{\\delta w_{j}} = -2\\rho +2w_{j} -  \\lambda   $$ \n",
    "$$ -2\\rho +2w_{j} -  \\lambda = 0 $$ \n",
    "$$  w_{j} = \\rho + \\lambda/2  $$\n",
    "\n",
    "if we need $ w_{j}<0 $ for this case, $ \\rho + \\lambda/2 <0 $ i.e. $\\rho < -\\lambda/2$\n",
    "\n",
    "#### Case 2: when $w_{j}$>0 \n",
    "\n",
    "$$ \\frac {\\delta RSS}{\\delta w_{j}} = -2\\rho +2w_{j} -  \\lambda   $$  \n",
    "$$ -2\\rho +2w_{j} -  \\lambda = 0  $$\n",
    "$$  w_{j} = \\rho - \\lambda/2  $$\n",
    "\n",
    "if we need $ w_{j}>0 $ for this case, $ \\rho - \\lambda/2 >0 $ i.e. $\\rho > \\lambda/2$\n",
    "#### Case 3: when $w_{j}$ = 0\n",
    "$$ w_{j}>0 : \\rho > \\lambda/2 $$\n",
    "$$ w_{j}<0  : \\rho < -\\lambda/2 $$\n",
    "\n",
    "$w_{j}$ = 0 : everything in between these bounds: $ [\\rho < -\\lambda/2, \\rho > \\lambda/2 ] $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$w_j = \\rho_j + \\lambda/2 \\hspace{0.5cm} if \\hspace{0.5cm}\\rho < -\\lambda/2 $$\n",
    "$$w_j = 0   \\hspace{1cm}\\hspace{0.5cm}if \\hspace{0.5cm} \\rho   [-\\lambda/2, \\lambda/2] $$\n",
    "$$w_j = \\rho_j - \\lambda/2\\hspace{0.5cm}  if \\hspace{0.5cm}\\rho > \\lambda/2 $$\n",
    "\n",
    "![](helper/L7.JPG) \n",
    "\n",
    "\n",
    "There is range $(-\\lambda/2, \\lambda/2)$ where we make our weights zero (as opposed to ridge in red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

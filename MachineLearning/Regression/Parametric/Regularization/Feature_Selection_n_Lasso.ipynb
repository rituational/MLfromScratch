{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Feature Selection\n",
    "\n",
    "1. Efficient computation (if we have say 1 billion features)\n",
    "2. Easy interpretation \n",
    "\n",
    "\n",
    "## Feature Selection Methods\n",
    "\n",
    "### All Subset Model\n",
    " \n",
    "We start with zero features (random noise) and increase number of features and build a model for each of these combinations. \n",
    "Eg. For no. of features = 1, we build a model for each features seperately. Then for no. of features = 2, we build model for all combinations of 2 features and so on. At the end we select the model with the best cross validation score. \n",
    "![](./helper/L1.JPG)\n",
    "\n",
    "(the features in best model for k feature combinations neednt have features in best model using k-1 feature combinations.\n",
    "\n",
    "Computational Complexity: \n",
    "for dataset with D features, $ 2^{D+1}$ models needed to be evaluated. \n",
    "\n",
    "### Greedy Methods\n",
    "\n",
    "#### Forward stepwise search\n",
    "\n",
    "At each step of all subset, we select the feature that performs the best and then combine that with remianing feature and so on. So if we selected sq. feet area in the first feature iteration, next time we freeze sq feet and for no. of features = 2, make combinations of sq feet with remaining features and so on. This one is greedy, hence wont necessarily arrive at the optimal solution globally.\n",
    "![](./helper/L2.JPG)\n",
    "\n",
    "\n",
    "\n",
    "Computational complexity At every step we have D, (D-1), .... 1 (= D(D-1)/2 ~ #D^2$) options. $O(D^2) < 2^{D+1} $\n",
    "\n",
    "\n",
    "There are other methods like backward stepwise selection, combination of both etc. \n",
    "\n",
    "\n",
    "\n",
    "# Lasso\n",
    "### Why Lasso:      \n",
    "Because ridge doesnt make coeff go exactly zero but very close to zero. \n",
    "\n",
    "But whats the problem with near zero and not zero! \n",
    "* sparse input matrix are faster to compute and efficient. \n",
    "* Less number of features implies better interpretability.\n",
    "\n",
    "Lasso knocks out features by making the coefficients zero.\n",
    "\n",
    "### Why Ridge + Thresholding doesnt work: \n",
    "##### (With Correlated Features)\n",
    "\n",
    "In ridge we have square of weights to be minimized. Thus, if we have two highly correlated features, rather than assigning a large weight to one feature and zero to another, it will assign small weights to both. (4^2+4^2 = 32 , 8^2+0 = 64. while minimizing for loss, 32 looks better) \n",
    "\n",
    "If we now put a threshold and knock off small coefficients both the correlated coefficient will be knocked off. At least one should be preserved, its valuable information getting lost. \n",
    "\n",
    "\n",
    "eg. In house price prediction we have num of bathrooms & num of showers, highly correlated. Ridge assigns small weights to both (in green). But, let us say number of showers wasnt present in the dataset, ridge would have given higher coefficient to number of bathroom (in red), in that case thresholding might not have knocked it off, but it does now when both are present\n",
    "\n",
    "![](./helper/L3.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Cost Function \n",
    "\n",
    "Hence, lasso objective is to balance the fit(RSS) and also sparsity\n",
    "$$ Cost = RSS(w) + \\lambda ||W||_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient Path for Ridge vs Lasso\n",
    "\n",
    "[Ridge vs Lasso coefficients for housing predictions](Ridge_vs_Lasso_Coefficients_Housing.ipynb)\n",
    "\n",
    "\n",
    "[Ridge vs Lasso coefficients for simple polynomial regression](Ridge_vs_Lasso_Coefficients_Poly_Reg.ipynb)\n",
    "\n",
    "Below we see the path the coefficients take in ridge and lasso. We can see lasso makes non influential features for pretty small alphas  \n",
    "\n",
    "\n",
    "![](./helper/L5.JPG)\n",
    "\n",
    "![](./helper/L6.JPG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

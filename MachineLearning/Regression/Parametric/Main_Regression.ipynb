{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression \n",
    "\n",
    "###  Linearity \n",
    "* y is linearly related to x \n",
    "\n",
    "\n",
    "### Homoscadascity\n",
    "* Variance of residuals is same for any value of predicted value \n",
    "* $y^{\\cap} = w_{0} +w_{1}x + \\epsilon $ \n",
    "\n",
    "$\\epsilon $ is equally likely to be above or below the predicted line \n",
    "![](./helper/1_2.JPG)  \n",
    "\n",
    "\n",
    "### Independence \n",
    "* Observations are independent of each other. \n",
    "eg. We cant use LR on time series because each observation may depend on the past values\n",
    "\n",
    "\n",
    "### Normality \n",
    "* The residuals have normal distribution \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Formulation\n",
    "\n",
    "$$ \\hat y = {\\theta}^{T}.X $$\n",
    "\n",
    "\n",
    "$ \\hat y $= prediction\n",
    "\n",
    "\\theta are feature weights\n",
    "\n",
    "and X is array of features\n",
    "\n",
    "\n",
    "\n",
    "$ y = h(\\theta.x) + \\epsilon $        $\\epsilon$ is irreducible error\n",
    "\n",
    "To train we need to find y-$ \\hat y $ (cost function) Our loss function is RSS (residual some of squares). We can also use mean square error (MSE) which is average of RSS.\n",
    "\n",
    "$$ RSS = \\sum({y- \\hat y})^2 $$ \n",
    "\n",
    "$$ MSE = \\frac{1}{m}\\sum({y- \\hat y})^2 $$    where m is total number of observations \n",
    "\n",
    "\n",
    "\n",
    "In matrix form \n",
    "$$  Loss Function = \\frac{1}{2m}(Y-\\hat Y)^T(Y-\\hat Y) $$ #Note: the denominator may may not have 2 depending on formulation (no consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving LR\n",
    "\n",
    "### Normal/Closed Form Solution\n",
    "\n",
    "[Derivation and limitations](./LinearRegression/LR_OLS.ipynb)\n",
    "\n",
    "\n",
    "Differentiating loss function and equating to zero, we get\n",
    "\n",
    "$$ \\theta = (X^{T}X)^{-1}X^{T}Y $$ \n",
    "\n",
    "Note: The inverse will exist if no. of independent observations > no. of features, and features are independent of each other. \n",
    "\n",
    "\n",
    "Adv:\n",
    "* Direct solution. No hyper parameter tuning required.\n",
    "\n",
    "Disadv:\n",
    "Computational Complexity:\n",
    "* $(X^T X)^{-1}$ has Computational Complexity of $O(m^{2.4})$ to $O(m^3)$. It increases with increase in number of features, number of observations.\n",
    "For ex: if you double the number of features, the time increases by $2^{2.4}$ to  $2^{3}$\n",
    "* For pseudo inverse, SVD has Complexity of $O(m^2)$\n",
    " \n",
    "### Gradient Descent\n",
    "\n",
    "[Derivation](./LinearRegression/LR_Gradient_Descent.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "$$ while ||RSS||   <\\epsilon: $$\n",
    "$$ \\theta = \\theta + \\frac{\\alpha}{m}(Y-\\hat Y)X^{T} $$\n",
    "\n",
    "**Interpretation of the formula:**\n",
    "\n",
    "\n",
    "Eg. House price vs no. of rooms. We initially start with a small weight that underpredicts, so $Y-\\hat Y$ is positive on average. What will decrease the residual? Increase $ \\hat Y$ For that weight should be higher. \n",
    "\n",
    "So, positive error --> increase weights (hence + sign)\n",
    "\n",
    "For weight update, we multiply our gradient by the respective feature (intuition: We want gradient to have high impact on = feature with more magnitude over our iteration)\n",
    "\n",
    "\n",
    "* Adv: \n",
    "Well suited when we have large number od observations, features.\n",
    "* Disadv: \n",
    "Tune learning rate\n",
    "\n",
    "\n",
    "[Hands On for GD and OLS vs Sklearn](./LinearRegression/LR_Hands_On_GD_OLS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting LR Results\n",
    "\n",
    "A particular coefficient $w_{i} $ can be interpreted as change in y w.r.t unit change in $x_{i}$ with all other x at some constant value. \n",
    "Eg. In house prediction, it may happen that we have square feet, no of rooms etc. The coefficient for no of room may be negative. This can be counterintuitive. We expect higher price for higher number of rooms. But since we also have square feet as a feature, if we hve constant sqaure and have more number of rooms, such house with many tiny rooms may have less price. Thus, the coefficient of no. of rooms might be negative. In case we built our model with only no. of rooms, it may have positive coefficient. Thus, coefficients dont make sense in silos but when seen together with others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be some points in the dataset which donot follow general trend and can influence our predictions. Mainly: \n",
    "\n",
    "\n",
    "### High Leverage Points \n",
    "Extreme values of x where there are no other observations.Heavily effects the least square line, as centre of mass of x heavily influenced by that point. If high leverage point follows the trend of other data, that may not cause big problem. But if doesnt, can heavily influence the resulting fit. \n",
    "\n",
    "### Influential Observation\n",
    "Ones removing which changes the fit significantly. High leverage points are one candidate for this. But observations with x in normal value but y having strong outliers are one of them.\n",
    "\n",
    "[Influential points hands on](./LinearRegression/LR_High_Leverage_Points.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "\n",
    "In simple linear regression, we had y = w*x. We dont need to be restricted to only x but can derive features from our inputs. So this can be written as  \n",
    "\n",
    "$$ \\hat y = h(\\theta.x)= {\\theta}^{T}.X $$\n",
    "\n",
    "(every feature may/may not be dependent on all the x values.)\n",
    "\n",
    "\n",
    "### Time Series \n",
    "\n",
    "Eg. In time series data, to model the seasonality we can have a sin(x) term. $ \\phi$ is the phase which controls the start of seaonality. We can keep adding multiple features dervied from x (**Doubt:** so the assumption that observations are independent doesnt need to hold now?? )\n",
    "\n",
    "$ y = w1x+w2sin(2pix+\\phi) $\n",
    "\n",
    "### Polynomial Regression \n",
    "We create features from product of original input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error\n",
    "\n",
    "#### Training Error \n",
    "Loss in training set. Might not be a good represntation unless training data includes entire population.\n",
    "\n",
    "#### Generalization error           \n",
    "What we really want. Ideal out of model error when we have the entire population data. But we dont have all of them. \n",
    "\n",
    "[notes](Errors/Errors.ipynb)\n",
    "\n",
    "#### Testing error       \n",
    "What we can actually compute\n",
    "\n",
    "\n",
    "\n",
    "### 3 Sources of error\n",
    "\n",
    "#### Bias (reducible)\n",
    "\n",
    "* low complexity -> high bias\n",
    "* Inflexibility of our model to capture the true relationship. \n",
    "\n",
    "#### Variance (reducible) \n",
    "\n",
    "* Low complexity model -> low variance\n",
    "* How sensitive is the model to the data samples considered. \n",
    "\n",
    "#### Noise (irreducible)          \n",
    "Data inherently noisy. Cant remove this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting And Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to observe overfitting\n",
    "\n",
    "We can find whether a model is overfitting (not generalizing well) by checking the train and test time loss. \n",
    "But is there any other thing which is represntative of overfitting? \n",
    "\n",
    "In [Relation of overfitting and weight magnitude](./Regularization/Overfitting&Weights.ipynb)   we observe that when model overfits, its coefficients become very large. \n",
    "\n",
    "Overfit can occur due to two scenarios : \n",
    "1. With large number of features (Model has lot of flexibility to explain the data)\n",
    "2. With increased polynomial order in a feature (same reason)\n",
    "\n",
    "How number of obs affect overfit:           \n",
    "1. if small data : rapid overfit as model complexity increases.                \n",
    "2. if large data : Hard to go over all data, so harder to overfit  \n",
    "\n",
    "We need to reduce the coefficients values and inturn keep the model simple and yet maintain similar bias.\n",
    "\n",
    "Thus the Cost Equation : Measure of Fit (Small implies good Fit) + Measure of Coeff Magnitude (Small implies not overfit)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to quantify the coeff magnitude?            \n",
    "1. Sum :  Doesnt hold good if one coeff is >> 1 and other is << 1. The sum becomes zero.            \n",
    "2. Sum absolute value (L1 Norm):\n",
    "$$ |w_0| + |w_1| ... |w_n|  = ||W||_1$$         \n",
    "3. Sum of Squares (L2 Norm)\n",
    "$$ w_0^2 + w_1^2 ... w_n^2  = ||W||^2_2$$      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "[Detailed_Derivation](./Regularization/Ridge_Regression_Derivation.ipynb)\n",
    "\n",
    "\n",
    "$$ Cost = RSS(w) + \\lambda ||W||^2_2 $$\n",
    "\n",
    "\n",
    "### Case 1 : Closed form Solution \n",
    "\n",
    "In OLS for XX^T matrix to be invertible we need No. of observations > no. of features             \n",
    "\n",
    "Ridge, **closed form solution always exists provided $\\lambda$ > 0**, can work with  No. of observations < no. of features as well \n",
    "\n",
    "\n",
    "$$ w = (X^TX + \\lambda I)^{-1} X^Ty $$\n",
    "\n",
    "### Case 2 : Gradient Descent\n",
    "\n",
    "$$ w = (1 - 2\\alpha \\lambda)w + 2\\alpha *[(Y- \\hat Y)X^T] $$\n",
    "\n",
    "at every step we first reduce our weight to $(1 - 2\\alpha \\lambda)w $ and then add the RSS component. Thus,  we make sure to reduce model complexity by reducing magnitude of weights. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevent Regularization of Intercept \n",
    "\n",
    "If we use the generic regularizer equation, it penalizes the intercept as well (ie, shrinks the intercept!). Which need not be correct always. \n",
    "So we may exclude weight corresponding to intercept in our regularization. \n",
    "\n",
    "( Above examples taking the case of ridge )\n",
    "#### Option 1: \n",
    "Dont add $W_0^2$\n",
    "$$ W_0 = W_0 - \\alpha \\nabla cost $$\n",
    "$$ W_j = (1 - 2\\alpha \\lambda)W_j - \\alpha \\nabla cost $$\n",
    "j > 0 (remaining features)\n",
    "\n",
    "#### Option 2: With Centering       \n",
    "When you center about 0, then small intercept doesnt matter. You can proceed  as normal. \n",
    "Steps:          \n",
    "1. Transform y to have 0 means          \n",
    "2. Run ridge as normal (closed form/ GD)\n",
    "\n",
    "\n",
    "[same_text](./Regularization/Normalize_Input_For_Regularization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Feature Selection\n",
    "\n",
    "1. Efficient computation (if we have say 1 billion features)\n",
    "2. Easy interpretation\n",
    "\n",
    "Some Techniques: \n",
    "### All Subsets \n",
    "We start with zero features (random noise) and increase number of features and build a model for each of these combinations. \n",
    "Eg. For no. of features = 1, we build a model for each features seperately. Then for no. of features = 2, we build model for all combinations of 2 features and so on. At the end we select the model with the best cross validation score. \n",
    "\n",
    "\n",
    "Disadv: For D features, $ 2^{D+1}$ models to be evaluated.\n",
    "\n",
    "### Greedy Methods \n",
    "#### Forward selection\n",
    "At each step of all subset, we select the feature that performs the best and then combine that with remianing feature and so on. \n",
    "At every step we have D, (D-1), .... 1 (= D(D-1)/2 ~ $D^2$) options. $O(D^2) < 2^{D+1} $\n",
    "\n",
    "There are other methods like backward stepwise selection, combination of both etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression \n",
    "\n",
    "### Why Ridge + Thresholding doesnt work: \n",
    "\n",
    "In ridge we have square of weights to be minimized. Thus, if we have two highly correlated features, rather than assigning a large weight to one feature and zero to another, it will assign small weights to both. (4^2+4^2 = 32 , 8^2+0 = 64. while minimizing for loss, 32 looks better) \n",
    "\n",
    "If we now put a threshold and knock off small coefficients both the correlated coefficient will be knocked off. At least one should be preserved, its valuable information getting lost. \n",
    "\n",
    "\n",
    "### Why Lasso\n",
    "Because ridge doesnt make coeff go exactly zero but very close to zero. If majority of coefficients are zero:\n",
    "* sparse input matrix are faster to compute and efficient. \n",
    "* Less number of features implies better interpretability.\n",
    "\n",
    "Lasso knocks out features by making the coefficients zero.\n",
    "\n",
    "### Cost Function \n",
    "\n",
    "$$ Cost = RSS(w) + \\lambda ||W||_1 $$\n",
    "\n",
    "[Lasso Derivation](Lasso_Regularization_Derivation.ipynb)\n",
    "\n",
    "|W| isnt differentiable, hence no closed form solution. \n",
    "\n",
    "We use coordinate descent. Instead of computing gradients for all features together, we minimize cost function one coefficient at a time and keeps others fixed. Then we moves to other features and follow same method recursively. \n",
    "\n",
    "For least squares solution using coordinate descent looks like \n",
    "\n",
    "$$ w_{j} = \\rho $$\n",
    "$$ \\rho = -2\\sum \\limits _{i =0} ^{N} h_{j}(x)(y_{i}- \\sum \\limits _{k != j} w_{k}h_{k}x)  $$ \n",
    "\n",
    "#### Coordinate Descent\n",
    "* No learning rate hyperparameter. \n",
    "* we minimize cost function one coefficient at a time and keeps others fixed. Do this for each feature. \n",
    "* Stopping criteria: Cycle through all coordinates and converge max steps <$\\epsilon$ (max step means the jump we took at each iteration)\n",
    "* Intuition of the equation, we check the residuals without $ j^{th} $ term. If our residual is large, that means this feature is important hence we assing that large value to weight. Again we multiply by the feature value because we want our update to be proportional to that\n",
    "\n",
    "\n",
    "$$w_j = \\rho_j + \\lambda/2 \\hspace{0.5cm} if \\hspace{0.5cm}\\rho < -\\lambda/2 $$\n",
    "$$w_j = 0   \\hspace{1cm}\\hspace{0.5cm}if \\hspace{0.5cm} \\rho   [-\\lambda/2, \\lambda/2] $$\n",
    "$$w_j = \\rho_j - \\lambda/2\\hspace{0.5cm}  if \\hspace{0.5cm}\\rho > \\lambda/2 $$\n",
    "\n",
    "![](Regularization/helper/L7.JPG) \n",
    "\n",
    "[Ridge vs Lasso w.r.t Coefficients](Ridge_vs_Lasso_Coefficients_Poly_Reg.ipynb)\n",
    "\n",
    "\n",
    "[Feature_Selection_n_Lasso](./Regularization/Feature_Selection_n_Lasso.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualing Contours\n",
    "\n",
    "### Ridge \n",
    "![title](./Regularization/helper/Ridge_Contour.PNG)\n",
    "\n",
    "$$ RSS(w) = \\sum (y_i - w_0h_0 - w_1h_1)^2 $$ \n",
    "(Equation of ellipse) \n",
    "\n",
    "$$ Ridge cost = \\lambda (w_0^2 + w_1^2)$$\n",
    "(Equation of circle)\n",
    "\n",
    "\n",
    "\n",
    "* The mid point of ellipse (b) is the optimal point w.r.t. RSS. Origin (a) is optimal w.r.t L2 reg. \n",
    "* We need to find the intersection of these two curves, since both terms present in our cost function.\n",
    "* Curve R2 has lower RSS, but higher L2 penalty. R1 has higher RSS but less L2 penalty. \n",
    "\n",
    "\n",
    "### Lasso\n",
    "![title](./Regularization/helper/Lasso_Contour.PNG) \n",
    "\n",
    "In lasso, the probability to hit the sharp corner of diamond shape is higher than smooth circular contour for ridge. Thus, Lasso quickly approaches to sparse solution (features coef going to zero).\n",
    "\n",
    "[same text](./Regularization/Visualize_Ridge_Lasso.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.Video object>"
      ],
      "text/html": "<video src=\"./Regularization/helper/ridge_intiution_video.mp4\" controls  >\n      Your browser does not support the <code>video</code> element.\n    </video>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"./Regularization/helper/ridge_intiution_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.Video object>"
      ],
      "text/html": "<video src=\"./Regularization/helper/lasso_intiution_video.mp4\" controls  >\n      Your browser does not support the <code>video</code> element.\n    </video>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"./Regularization/helper/lasso_intiution_video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Disadv:\n",
    "### Debiasing Lasso\n",
    "Lasso shrinks weights and can potentially cause high bias situation. To avoid that:\n",
    "1. Run lasso to select optimal features         \n",
    "2. Run least sq with those selected features. Implying those feature coefficients wont be shrunk relative to lasso output.\n",
    "\n",
    "\n",
    "### Correlated Variables:\n",
    "If you have a collection of strongly correlated features, lasso will tend to just select amongst them pretty much arbitrarily. A small tweak in the data might lead to change in variable included. Eg In housing price prediction, square feet and lot size \n",
    "\n",
    "### Ridge outperforms Lasso\n",
    "It's been shown empirically that in many cases, ridge regression actually outperforms lasso in terms of predictive performance. \n",
    "\n",
    "Elastic net: Fused ridge and lasso objectives\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic net\n",
    "\n",
    "$$ Penalty = \\alpha||w||_{2} + (1-\\alpha)|w|_{1} $$\n",
    "\n",
    "![](Regularization\\helper/EN1.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: handson for debiasing lasso, check if  ridge outperforms lasso in terms of MSE? \n",
    "### MSE vs RMSE vs MAE vs Huber \n",
    "#### R2 adjusted R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3612jvsc74a57bd06a33784d2e02aee83e49ac6533fd5586adcd80ae4d6db07a7c73263e9bf81256",
   "display_name": "Python 3.6.12 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
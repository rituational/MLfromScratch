{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation to move away from Linear Regression\n",
    "\n",
    "Consider x and y where the relationship is not clear that initially it is linear, midway it constant and towards end it is quadratic. It is difficult to model these changing behaviour well through linear regression. Polynomial regression could capture them, but again at the extent of high variance. \n",
    "\n",
    "We want to\n",
    "\n",
    "Hence, KNN type of non-parametric approaches work well in such scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D KNN:\n",
    "You are predicting price of house based on sqft. As per below figure you dont build any parametric model, rather you find the closest match in price for the sqft rate of interest.\n",
    "\n",
    "Pick a Xq and go to left and right along y and choose the prediction.\n",
    "\n",
    "![title](Images\\KNN_Concept.PNG)\n",
    "\n",
    "### 1D KNN in multiple dimesions:\n",
    "Voronoi Tesselation : Divide space into N regions each containting single datapoint. Where any predicted example lying in the space, distance is calculated to nearest all data points. The prediction is given as per min distance.\n",
    "![title](Images\\KNN_Concept_1D.PNG)\n",
    "\n",
    "## Distance Metrics\n",
    "\n",
    "Euclidean Distance: the usual Distance using shortest minimum distance between two points.          \n",
    "Manhattan Distance : just think of New York and driving along the streets of New York. It's measuring distance along this axis-aligned directions, so it's distance along the x direction plus distance along the y direction which is a different difference than our standard Euclidean distance\n",
    "\n",
    "### 1NN algorithm : \n",
    "1. Initialize the prediction to $\\inf$.            \n",
    "2. Loop over all data points:        \n",
    "    2.1 calculate the distance from test data to that data point.           \n",
    "    2.2 If the value is lower than stored prediction, replace with new value\n",
    "\n",
    "### Problem with 1NN:\n",
    "1. Sensitive to regions with little data. it will have to choose to the extreme left where data was there. It doesnt interpolate.\n",
    "2. Sensitive to noise in data. As it predicts with same noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN  :\n",
    "More reliable if you estimate from larger data. Reduces noise\n",
    "\n",
    "Algorithm:\n",
    "1. Choose first k input data, calculate their distance to test data point and sort\n",
    "2. This serves as K nearest neighbours. \n",
    "3. Now loop over from k+1:N in input data, if any of the data has distance lower than max distance in existing KNN, then update\n",
    "4. Find the index to update that data point to and also remove the last value in KNN group.\n",
    "5. Iterate this till end of data points.\n",
    "6. prediction : Average of k target value       \n",
    "\n",
    "Small k : Noisy predictions.        \n",
    "Large k : larger class will always be predicted and will be out-weighed from smaller category.\n",
    "\n",
    "## Problem with KNN:\n",
    "1. Boundary Effects: At start and end of data, the predictions are constant (boundary effect), bcos towards the start and end the nearest neighbours doesnt change, so we get same predicted value.\n",
    "2. Disconuity still exists. (Minor up and down).It is problematic esp when a unit change in x, can cause big change in y (if there is a discontinuity). Mainly bcos some neighbours keep moving in & out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted KNN\n",
    "Weigh more similar data more than those less similar in KNN. Reduce the weights of neighbours moving in & out as move over data.\n",
    "\n",
    "Weigh more : If the distance is small.      \n",
    "Weigh small : If the distance is large.  \n",
    "Weight Choice = 1/distance          \n",
    "\n",
    "Typical Weight : $Kernel_\\lambda(|X_{NN} - X_{pred}|)$            \n",
    "You tend to choose the weight based on kernel (like guassian), ie when distance of guassian kernel is zero, the weight is maximum & if d high then u go extreme with low weight. $\\lambda$ can choose how fastly u go to lower value (the heap part of guassian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted KNN to Kernel Regression\n",
    "\n",
    "Instead of weighing only K specific neighbours, you choose the neighbours based on kernel. Only subset of data falling from kernel boundary are needed to compute the fit.      \n",
    "Lot smoother than KNN.\n",
    "\n",
    "![title](Images\\Kernel_Regression.PNG)\n",
    "\n",
    "Choice of kernel and $\\lambda$ is crucial. Typically choice of $\\lambda$ more crucial than choce of kernel\n",
    "![title](Images\\Kernel_Regression_Lambda.PNG)\n",
    "\n",
    "Choosing $\\lambda$ : Bias_Variance Tradeoff - Cross Validation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models are local fit models compared to global fit in linear regression. Local fit implies we are fitting with subset of data based on NN. Even piecewise linear is local fit. \n",
    "\n",
    "Kernel Regression is local fit type algorithm and also avoids constant edge predictions through weighted NN predictions.\n",
    "\n",
    "The edge predictions can be even improved through local regression (with linear or polynomial fits)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric Approaches: \n",
    "KNN and kernel regression are examples\n",
    "\n",
    "Properties of Non-parametric models:        \n",
    "1. Flexibility.     \n",
    "2. Makes few assumptions about f(x)         \n",
    "3. Complexity grows with data (as u get more data, u need to perform similar iterations)            \n",
    "4. When you get more data:  \n",
    "    4.1 Noiseless data: you will get exact predictions This is not the same case with quad fit or other polynomial fit. There will some level of functional curve which we may not be able to fit through parametric approach. Ex: a quad fit cannot form a sine curve exactly regardless of how many points u provide.         \n",
    "    4.2 Noisy Data: Error can go to zero in KNN provided k grows along with data. Effect of noise will be smoothened with larger k with larger data.\n",
    "\n",
    "## Problems with non-parametric models:     \n",
    "1. Large dimension or Small Data:\n",
    "    Large d means you need large points to cover the space. Which is not good for performance. parametric models are good in that aspect\n",
    "2. Complexity in prediction:\n",
    "    To get one prediction u scan thro entire data. Inc with number of data and intensive. \n",
    "\n",
    "Better approaches are clustering and retrieval type models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Parametric Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./helper/global_vs_local.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, we fit our model to the whole of the data. To capture all the variance in the data, sometimes we would choose a high variance model (shown in left). Even though the points in the middle of the plot may be captured better with a constant line, we are forced to use polynomial as we are looking at the data at a global level. \n",
    "\n",
    "The plot to right fits the data at local level, and rather than having an equation describing the plot, this can be non parametric. \n",
    "\n",
    "But to get to that, we might want to divide our data into pockets, but we dont want to explicitly deal with it. Rather, we just look at the nieghbourhood of the query point and take decision based on the logic given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Nearest Neightbour \n",
    "\n",
    "<img src=\"helper/K1.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "For any query point $ x_{q} $ \n",
    "Find point closest in distance \n",
    "\n",
    " \n",
    "$$ X_{NN} = \\underset{i}{\\min}   distance(x_{i}, x_{q} )  $$\n",
    "\n",
    "\n",
    "$$ \\hat y_{q} = y_{NN} $$ (y corresponding to x closest to q) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 NN in 2 dimensional data  \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"helper/K2.PNG\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Voronoi Tesselation : Divide space into N regions each containting single datapoint. Where any predicted example lying in the space, distance is calculated to nearest all data points. The prediction is given as per min distance.\n",
    "\n",
    "For 2D data (and similar plot can be imagined in higher dimensions) we can visualize our plane segmented into N regions based on N data points. \n",
    "We dont actually need to do this division. Rather for every query point, we just calculate the point with minimum distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics \n",
    "\n",
    "We are to compute distance between datapoints, we used euclidean distance in the 1D, 2D example above. But we have many other choices. \n",
    "\n",
    "<br>\n",
    "\n",
    "Eg:\n",
    "* Scaled Eucledian Distance\n",
    "    * For our housing price prediction, e may want to give feature importance, so we can weigh our distances for each feature differently \n",
    "    * $$ distance(x_{i},x_{q}) = \\sqrt{a_{1}(x_{j}[1]-x_{q})^2 + a_{j}(x_{j}[i]-x_{q})^2 + a_{d}(x_{j}[d]-x_{q})^2}$$\n",
    "     where d = no. of features \n",
    "* Manhattan Distance\n",
    "    * Imagine you are driving on the streets of New York. So you find distance along x, then y; this is different than euclidean which will just take the diagonal. \n",
    "    * Other choices: Mahanobis, Hamming, cosine simiarity etc \n",
    "    \n",
    "#### Predictive Surfaces change with the metrics chosen \n",
    "As shown below \n",
    "\n",
    "\n",
    "<img src=\"helper/K3.JPG\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 NN Pseudo Code \n",
    "\n",
    "dist1NN = $\\infty$\n",
    "\n",
    "for i = 0, 1,... N: <br>\n",
    "    $ \\delta = distance(x_{i},x_{q}) $ <br>\n",
    "    $\\qquad$ if  $ \\delta  $ < dist1NN:<br>\n",
    "    $\\qquad$$\\qquad$ dist1NN = $\\delta$ <br>\n",
    "    $\\qquad$$\\qquad$ $X_{NN} = x_{i}$\n",
    "   \n",
    "$ Y_{NN} = y_{i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 NN Disadvantage\n",
    "\n",
    "1 NN **doesnt interpolate well** as shown below \n",
    "\n",
    "<img src=\"helper/K4.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "**Sensitive to noise** in data. (high variance model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbours\n",
    "\n",
    "To reduce the effect of noise, we can look at k neighbours rather than only one. (Real estate agents may look for multiple similar houses and give you average house price rather than looking for just one closest house)\n",
    "\n",
    "### K NN Pseudo Code \n",
    "\n",
    "\\# initialize with first k entries in dataset (sorted in ascending distance to query point : $X_{NN_k} $ is farthest)<br><br>\n",
    "$ X_{NN_1}, X_{NN_2},.. X_{NN_k} = x_{1},  x_{2},...  x_{k} $ <br>\n",
    "$ DistNN_1, DistNN_2,.. DistNN_k = sorted list(\\delta(1,j), \\delta(2,j) ...\\delta(k,j)) $  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "for i = k+1,... N: <br>\n",
    "$\\qquad$ \n",
    "    $ \\delta = distance(x_{i},x_{q}) $ <br>\n",
    "    $\\qquad$$\\qquad$ if $\\delta < DistNN_k: $ <br>\n",
    "    $\\qquad$$\\qquad$$\\qquad$ find j s.t. $ \\delta> DistNN_j$  but $\\delta< DistNN_{j+1} $ <br>\n",
    "    $\\qquad$$\\qquad$$\\qquad$ insert $ x_{i}$ at $X_{NN_j}$  shift by one j+1, ... k  <br>\n",
    "    $\\qquad$$\\qquad$$\\qquad$ insert $\\delta $ at $ DistNN_j $ shift by one j+1, ... k  <br>\n",
    "\n",
    "$ Y_{NN} $ = $\\frac{1}{N} \\sum $(y's for k most similar points found above)\n",
    "\n",
    "\n",
    "\n",
    "Salient points in pseudo code above: \n",
    "* initialize with first k entries in dataset (sorted in ascending distance to query point : $X_{NN_k} $ is farthest) \n",
    "* Check distance of query point for all k+1 to $N^{th} $ observation \n",
    "* At every step of this loop, check if distance is lesser than the farthest of the k neighbours \n",
    "* If yes, we need to update our k neighbours. But we shall check if this observation beats $(k-1)^{th}$ and so on. \n",
    "* So, in the sorted list we insert this observation at j (entries to left of it have distance lesser than $dist(i,q)$), and push everything else to the right. \n",
    "* at the end, we have indices of k nearest observations. We take average of the outputs and assign that to Y_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"helper/K5.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "\n",
    "So we take a mean of all the points in the yellow box to predict y. Thus, KNN has much smoother graph than 1 NN. But still, if the data is sparse, our interpolation is bad. \n",
    "\n",
    "Also, there are a lot of jumps. \n",
    "Eg. 2600 sq ft house is 50k, 2601 sq ft is 54k. We dont like to believe such models! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted K NN \n",
    "To overcome such jumpy predictions, we can weight the contribution of each k neihbours, the farthest having least effect. So even when we update and remove that observation, the effect is minimal an we get a smoother reponse. \n",
    "\n",
    "Note: We calculate the k neighbours using the algo given above. Here, we just do weighted average of y values. (here we arent iterating and finding the neighbours)\n",
    "\n",
    " $ \\hat y_q $ = $\\frac{1}{\\sum c_{j}} \\sum (c_{1}Y_{KNN_1} + c_{2}Y_{KNN_2}...  + c_{3}Y_{KNN_k}) $\n",
    "\n",
    "One choice for c = $ \\frac{1}{dist(X_{KNN_1}, x_{q})} $\n",
    "\n",
    "We can have kernels based on distances\n",
    "\n",
    "$$ C_{NN_j} = kernel_{\\lambda}(|X_{KNN_1} - x_{q}|) $$\n",
    "\n",
    "for multi dimensioal x, \n",
    "$$ C_{NN_j} = kernel_{\\lambda}(distance(X_{KNN_1}, x_{q})) $$\n",
    "calculated using any metrics defined above.\n",
    "\n",
    "\n",
    "eg. $ Gaussian_{\\lambda} = exp(-(X_{KNN_1}- x_{q})^2/\\lambda) $\n",
    "\n",
    "#### Note\n",
    "* high $\\lambda$, weights will decay slowly ?? \n",
    "* Weights never go exactly to zero for guassian kernel, they just become very, very small \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Regression \n",
    "\n",
    "Weigh all the available observations (rather than only computed K neighbours in KNN)\n",
    "\n",
    "$ \\hat y_q $ = $\\frac{\\sum \\limits _{i = 1} ^{N} (c_{qi}y_i)}{\\sum \\limits _{i = 1} ^{N}  c_{qj}}   $\n",
    "\n",
    "$ \\hat y_q $ = $\\frac{\\sum \\limits _{i = 1} ^{N} (kernel_{\\lambda}(distance(x_i, x_{q}))y_i)}{\\sum \\limits _{i = 1} ^{N}  kernel_{\\lambda}(distance(x_i, x_{q}))}   $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Effect of $\\lambda$ on prediction\n",
    "\n",
    "More than the choice of kernel, $\\lambda$ does the bias variance trade off. \n",
    "\n",
    "<img src=\"helper/K7.JPG\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "\n",
    "\n",
    "(Epanechnikov kernel used)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Below, we compare boxcar kernel (very similar to unweighted KNN) with Epanechnikov kernel. We see although boxcar has discontinuity the fit looks very similar to Epanechnikov kernel. <br>\n",
    "<br>\n",
    "<img src=\"helper/K8.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local vs Global Fit \n",
    "\n",
    "* KNN, Kernel regressions comes uner **Globally Weighted Averages** \n",
    "* We can also locally fit linear/poly regression, that is known as ** locally weighted regression**\n",
    "* Good choice: Lineat local fit \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"helper/K9.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbour with increasing observations \n",
    "\n",
    "#### Noiseless Data\n",
    "We see 1 NN can fit the true curve perfectly as the number of observations increase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./helper/1NN.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"./helper/1NN.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Data \n",
    "We need to have a high value of k to smooth over the noise as shown below \n",
    "\n",
    "<img src=\"helper/K11.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric vs Non Parametric \n",
    "\n",
    "\n",
    "<img src=\"helper/K10.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "\n",
    "In the limiting case of infinite amount of data \n",
    "* Non Parametric: True error (bias^2 + variance) goes to zero\n",
    "* Parametric (fixed model complexity): True error always has some bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of K NN \n",
    "\n",
    "* High dimensional data \n",
    "    * More the dimensions (in X) the more number of observation you need to cover the whole space (N = $ O(exp(D)) $)\n",
    "    * If some pockets have less observations, KNN fails to interpolate. \n",
    "    * Parametric models outperform in such cases.\n",
    "\n",
    "* Complexity of search with more observations\n",
    "    * For 1 NN, each query we search all N observations. (O(N))\n",
    "    * For k NN search complexity grows by O(Nlog(k))\n",
    "\n",
    "To get over these limitations we can sue clustering, retrieval methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K- NN for Classification\n",
    "\n",
    "Rather than taking average of k outputs, here we select the class which occurs the most in the k neighbours found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

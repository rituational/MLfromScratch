{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SVM\r\n",
    "\r\n",
    "* Imagine a 1D data, which is linearly seperable. Where should we keep the threshold. \r\n",
    "* Ideally, we should maximize the distance between the edges of each cluster: *Maximum margin classifiers* \r\n",
    "* Maximum margin classifiers are highly sensitive to outliers \r\n",
    "* So we go for *Soft Margins* We are ok with some misclassifications, we bank on Cross validation for best trade off. \r\n",
    "\r\n",
    "* *Support Vector Classifier*: soft margin classifier, the observations on edge, in the soft margin are called support vectors.\r\n",
    "* For 2D data we have lines, for 3D we have planes, for n D we call it *hyperplane* \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](helper/1.JPG)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In cases where the data isnt linearly seperable, we will have lot of misclassifications, regardless. \r\n",
    "![](helper/2.JPG)\r\n",
    "\r\n",
    "\r\n",
    "We use SVM: \r\n",
    "* We transform the data to a higher dimension and try to find a separation. Eg: In above case, we create $dosage^2$ as y axis. \r\n",
    "* But how to decide how tranform the data? We use kernels to systematically find a support vector classifier that works in higher dimension. \r\n",
    "* We used polynomial kernel (d = 1) in above case. We can use d=2, 3, n\r\n",
    "* Systematically increase the degree, calculate the nD relationship between each observation, use CV to find ideal d. \r\n",
    "\r\n",
    "![](helper/3.JPG)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel Trick \r\n",
    "* We calculate the high dimensional relation without actually transforming all data. \r\n",
    "\r\n",
    "### Polynomial Kernel \r\n",
    "We have r, d as hyper parameters. We derive that the dot product is representation of out kernel. For every data point, we just need to calculate the dot product. Rather we can just compute the $(a+b+r)^d$ Thus computation reduced. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](helper/4.JPG)\r\n",
    "![](helper/5.JPG)\r\n",
    "\r\n",
    "With decide r, d with CV. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Radial Basis Kernel Function: \r\n",
    "\r\n",
    "* Uses infinite dimension kernel\r\n",
    "* It acts like a weighted nearest neighbour, predicting based on the neighbourhood. \r\n",
    "* $ e^{\\gamma(a-b)^2)} $ $\\gamma$ is hyperparameter. \r\n",
    "* If $\\gamma$ high, small neighbourhood considered.\r\n",
    "* If $\\gamma$ small, large neighbourhood considered.\r\n",
    "\r\n",
    "\r\n",
    "Polynomial kernel: \r\n",
    "* $$ (a+b)^d = a^d * b^d $$\r\n",
    "\r\n",
    "Ref: https://www.youtube.com/watch?v=Qc5IyLW_hns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
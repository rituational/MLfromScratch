{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification \n",
    "\n",
    "### Basic Intuition\n",
    "\n",
    "As opposed to regression where we predicted some real value y, in classification we need to categorize an observation into classes. \n",
    "\n",
    "Eg. We want to predict whether a review was good or bad. We ave two classes positive, negative. Lets say we have two features: awesome, awful. Our prediction can be w1*(no. of awesome) + w2*(no. of awful). If this weighted sum of words is >0 we say the review is positive, if <0 we say negative. We can find w's which suit our data the best. We have a linear classifier. \n",
    "\n",
    "But ideally we would want to know how confident we are on our prediction. Eg. We are 55% confident that it was a positive review. So rather than {0,1} we can have probability which is always bounded between  \\[0,1\\].\n",
    "\n",
    "\n",
    "So we have our weighted sum, that can take values -$\\infty$ to +$\\infty$ and we would want a link function that squishes it to \\[0,1\\]. One such link function is sigmoid.  \n",
    "\n",
    "For a given fit, \n",
    "$$ P(y = 1|x_i, w_i) = \\hat y = \\frac{1}{1+e^{-W_ix_i}} $$\n",
    "$$ P(y = 0|x_i, w_i) = 1 - \\hat y = \\frac{e^{-W_ix_i}}{1+e^{-W_ix_i}}$$\n",
    "\n",
    "\\[\n",
    " P(y_i|x_i, w_i) = \n",
    "\\begin{cases} \n",
    "      \\frac{1}{1+e^{-W_ix_i}} & y=1 \\\\\n",
    "      1- \\frac{1}{1+e^{-W_ix_i}} & y=0 \n",
    "   \\end{cases}\n",
    "\\]\n",
    "\n",
    "To put this together in equation we can rewrite it as \n",
    "\n",
    "$$ P(y_i|x_i, w_i) = (\\hat y_i)^{y_i}(1-\\hat y_i)^{(1-y_i)} $$\n",
    "\n",
    "\n",
    "\n",
    "### Probability vs Likelihood\n",
    "\n",
    "**P (data| distribution) vs L(distribution| data)** \n",
    "\n",
    "* Eg. (probability) Given distribution ($\\mu _1, \\sigma _1 $) of indian weights, whats probability that ritika is 60kg?. \n",
    "    Indian weights distribution remains constant, we get different probability for different observations (weights)\n",
    "* Eg. (likelihood) Given ritika is 60kg, whats the likelihood that indian weights have certain distribution ($\\mu _2, \\sigma _2 $) \n",
    "    Our observation remains same, we get different likelihood for different distributions ($\\mu _x, \\sigma _x $) \n",
    "\n",
    "\n",
    "In our classification formulation, using sigmoid as our link function. \n",
    "\n",
    "If we have known w and x. We get different 'probabilities' for different outputs i.e. y=0, y=1.\n",
    "If we have some known observations x, and output y. We get different 'likelihoods' for different w's. \n",
    "\n",
    "## Formulation \n",
    "\n",
    "We have known N independent observations. Now, we want to find a function (i.e. right values of w's) which fits these observations the best! (i.e. for given $x_i$ we want to be 100% confident for class 1 if y_i = 1 and vice versa)\n",
    "\n",
    "Thus, we want to maximize our likelihood of w's given x,y. \n",
    "\n",
    "\n",
    "$$  =  P(y_i|x_i) $$ \n",
    "\n",
    "If for any point likelihood is l(w_i|x_i), likelihood of all the N independent observations (ob1&obs2...&obsN : multiply probability), \n",
    "\n",
    "$$ l(w|x_i) = \\prod_{i=0} ^{N}  l(w_i|x_i) $$  \n",
    "\n",
    "We want to maximize this quantity. For convenience, since log is monotonous, we can maximize log likelihood as well. \n",
    "\n",
    "$$ log.l(w|x_i) = log(\\prod_{i=0} ^{N}  l(w_i|x_i) $$  \n",
    "$$ ll(w|x_i) = \\sum \\limits_{i=0} ^{N}  log l(w_i|x_i) $$  (log(ab) = loga+logb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ ll(w) = \\sum \\limits_{i=0} ^{N} (\\hat y_i)^{y_i}(1-\\hat y_i)^{(1-y_i)} $$\n",
    "\n",
    "(This looks exactly like probability equation, but here w is variable. There w was fixed and we were finding y.)\n",
    "\n",
    "$$ ll(w) = \\sum \\limits_{i=0} ^{N} y.log(\\frac{1}{1+e^{-w^{T}x}})+(1-y)log(\\frac{e^{-w^{T}x}}{1+e^{-w^{T}x}}) $$\n",
    "$$ ll(w) = \\sum \\limits_{i=0} ^{N} -y.log(1+e^{-w^{T}x})+(1-y)log(e^{-w^{T}x})+(1-y)log(1+e^{-w^{T}x}) $$\n",
    "$$ ll(w) = \\sum \\limits_{i=0} ^{N}-(1-y)w^{T}x-log(1+e^{-w^{T}x}) $$\n",
    "\n",
    "\n",
    "For one data point, \n",
    "$$ \\frac{\\delta ll}{\\delta\\omega} = -(1-y)x-\\frac{e^{-w^{T}x}(-x)}{1+e^{-W^{T}x}}$$\n",
    "$$ \\frac{\\delta ll}{\\delta\\omega} = [-(1-y)x+(1-\\hat y)x]$$\n",
    "$$ \\frac{\\delta ll}{\\delta\\omega} = [x(y-\\hat y)]$$\n",
    "\n",
    "For all observations, matrix form becomes\n",
    "$$ \\frac{\\delta ll}{\\delta\\omega} = X^{T}.(Y-\\hat Y)$$\n",
    "\n",
    " \n",
    "\n",
    "Since we are maximizing likelihood, we incrementally increase the weight, also called gradient ascent.\n",
    "$$ \\omega = \\omega + \\alpha.\\frac{\\delta L}{\\delta\\omega} $$\n",
    "\n",
    "\n",
    "#### Note\n",
    "Mean square error isnt used in logistic regression [because the formulation is non convex ](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting in classification is twice as bad \n",
    "\n",
    "### overfit classifier -> overly confident predictions\n",
    "\n",
    "[Visualizing_Overfitting](detailed_notebooks/Visualize_Overfitting_in_LogReg.ipynb)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Maximum likelihood estimation prefers the model that is most certain. Thus, even for border line data points, where ideally we should have low cnfidence, MLE pushes it to either 0 or 1! This can cause severe overfitting which results in traits mentioned below: \n",
    "\n",
    "* Coefficients can go to infinity for linearly separable data. (If we have linearly seperable data, even if we multiply weights by a high value, the solution remians same eg. w1x + w0 = 0, 1000w1x+ 1000 = 0) MLE will prefer the weight that is highest (we maximize weights!)\n",
    "* Narrow range where model is uncertain. So we are really confident at places where we can really go wrong. \n",
    "* Overly complex decision boundaries. (if we have a lot of features, we will somehow get a plane that makes our data linearly seperable. And coefficients can go to infinity for this linearly separable data in high dimensional space). \n",
    "\n",
    "\n",
    "Simple log regression using one feature of iris data shows the following:\n",
    "<img src=\"helper/LC3.JPG\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "\n",
    "For the same iris dataset, if we use polynomial regression, we are even prone to overfitting. \n",
    "\n",
    "<img src=\"helper/LC4.JPG\" alt=\"Drawing\" style=\"width: 1200px;\"/> \n",
    "\n",
    "#### Observation \n",
    "\n",
    "In the fig 1, without any regularization, the sigmoid becomes very steep. So our region of uncertainity is really narrow. Also, the weights increase as we increase epochs. \n",
    "\n",
    "In fig 2, with increasing degree of polynomial our decision boundary gets overly complex. \n",
    "\n",
    "## Tackle overfitting with L2 regularization \n",
    "\n",
    "<br>\n",
    "To avoid overfiiting we need to add representation of magnitude of weight in our cost function in addition to the error. In L2 (Ridge) regularization we take L2 norm as given below:\n",
    "$$ ll(w) = \\sum \\limits_{i=0} ^{N}  log l(w_i|x_i) - \\frac{\\lambda }{2}||w_i||^2_2 $$\n",
    "\n",
    "(Since we are maximizing our likelihood, we **reduce** it by the regularization term\n",
    "\n",
    "Thus the GD update becomes:\n",
    "$$ w = w + \\alpha(\\frac{\\delta \\cal{L}}{\\delta \\omega} -\\lambda\\omega)$$ \n",
    "\n",
    "Note: in sklearn, C = 1/$\\alpha$\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"helper/LC2.JPG\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining sparsity with L1 regularization\n",
    "\n",
    "<br>\n",
    "\n",
    "[Detailed_code](detailed_notebooks/L1_Reg_Feature_importance.ipynb)\n",
    "<img src=\"helper/LC1.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/> \n",
    "\n",
    "* Case 1 - No Reg: All non zero coefficients \n",
    "* Case 2 - With L1 reg:  knocks down unimportant features. Plotting the features, we observe the classes look linearly seperable using these two features.This seems the right fit. \n",
    "* Case 3 - With heavy L1 reg: L1 reg retains only one (most important feature) as seen from the parallel plot. Although our test predictions are bad. So model has bias.\n",
    "* Case 4: With very heavy L1: All coeeficients are zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

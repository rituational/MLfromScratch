{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Trees\r\n",
    "* At \r\n",
    "\r\n",
    "### Regression trees\r\n",
    "* At every node, a decision boundary between each data point, calculate the boundary which has least sum of square of residuals. \r\n",
    "* The output is the average of all the observations in that resultant node. \r\n",
    "* If we have multiple features, follow step one for each of the features and then select the feature taht has minimum RSS. \r\n",
    "* Keep building the tree, until the number od observations in the child node is less than min_obs (usually 20) or the output for each observation = mean observation. \r\n",
    "\r\n",
    "![](helper/1.JPG)\r\n",
    "![](helper/2.JPG)\r\n",
    "![](helper/3.JPG)\r\n",
    "\r\n",
    "### Pruning \r\n",
    "\r\n",
    "#### Cost complexity Pruning\r\n",
    "Tree score has alpha which is a hyperparameter we train with cross validation. \r\n",
    "* We build a tree with all data (train+test) We then create trees for different $\\alpha$s, as $\\alpha$ increase, we prunethe tree more until we have only root.\r\n",
    "* Now we we do k fold cross val, and in each fold find the value of $\\alpha$ that works best for taht particular train, test data \r\n",
    "* We select $\\alpha$ that has minimum RSS. Then we go back and select the corresponding tree we built initially with the whole data. \r\n",
    "![](helper/4.JPG)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest \r\n",
    "\r\n",
    "Disadvantage of decision trees is inaccuracy: They work well for the training data but dont generalize well (variance?)\r\n",
    "\r\n",
    "1. Bootstrapped Dataset: Randomly select samples from original data with repetition, to get a dataset of same size as original. \r\n",
    "2. Use only a random subset of variables/ features to create a decision tree from the bootstrapped dataset. \r\n",
    "3. Make many such trees. At test time, take the aggregate of these weak learners\r\n",
    "\r\n",
    "This is known as bagging. \r\n",
    "\r\n",
    "* When we do bagging, there are some samples which werent included in the current bootstrapped dataset. Called *Out of bag samples* \r\n",
    "* We can measure the performance of our RF by proportional of out of bag correctly classified. \r\n",
    "* Proportion of out of bag incorrectly classified are called *out of bag error*.\r\n",
    "\r\n",
    "### Random Forest and missing data \r\n",
    "* For every missing data in training data, we take an initial guess and make decision trees. \r\n",
    "* We make proximity matrix to find which sample looks the closest (we keep a count of the prediction from each tree in the random forest.)\r\n",
    "* We predict the value of missing data based on a weighted average based on the proximity matrix for all the samples. \r\n",
    "\r\n",
    "* During inference we do something similar on the forest we have already built. \r\n",
    "\r\n",
    "* This proximity matrix gives us which samples are closer to each other and the inverse can give the distance which can be used to make a heatmap (and do some clustering)\r\n",
    "Better check this again in case https://www.youtube.com/watch?v=sQ870aTKqiM\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}